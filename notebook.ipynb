{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "GLOBAL = {\n",
    "    'embedded_length': 512,\n",
    "    'continuity_length': 64,\n",
    "    'n_vocab': 29\n",
    "}\n",
    "\n",
    "# Transformer (Top level)\n",
    "TRANS_CONST = {\n",
    "    'n_attention_layers': 8,\n",
    "    'n_attention_heads': 8,    \n",
    "\n",
    "    'max_output_length': 6,\n",
    "    \n",
    "    'embedding_dic_size': GLOBAL['n_vocab'], \n",
    "    'embedded_vec_size': GLOBAL['embedded_length'],\n",
    "    \n",
    "    # 'pos_encoding_input': GLOBAL['embedded_length'],\n",
    "    # 'pos_encoding_output': GLOBAL['embedded_length'],\n",
    "    \n",
    "    'linear_input': GLOBAL['embedded_length'],\n",
    "    'linear_output': GLOBAL['n_vocab'] # output vocab size\n",
    "}\n",
    "\n",
    "# Encoder, EncoderLayer\n",
    "ENCODER_CONST = {\n",
    "    'norm1_size': GLOBAL['embedded_length'], # same as input matrix width\n",
    "    'norm2_size': GLOBAL['embedded_length'],\n",
    "\n",
    "    # maybe rename these two, it's just for knowing the input dim and the dim that the FF layer will work with\n",
    "    'ff1': GLOBAL['embedded_length'], \n",
    "    'ff2': GLOBAL['embedded_length'] * 4\n",
    "}\n",
    "\n",
    "# Decoder, DecoderLayer\n",
    "DECODER_CONST = {\n",
    "    'norm1_size': GLOBAL['embedded_length'], # same as input matrix width\n",
    "    'norm2_size': GLOBAL['embedded_length'],\n",
    "    'norm3_size': GLOBAL['embedded_length'],\n",
    "\n",
    "    'ff1': GLOBAL['embedded_length'],#TODO RENAME\n",
    "    'ff2': GLOBAL['embedded_length'] * 4#TODO RENAME\n",
    "}\n",
    "\n",
    "# MultiHeadAttention, SingleHeadAttention\n",
    "ATTENTION_CONST = {\n",
    "    'mh_concat_width': GLOBAL['continuity_length']*TRANS_CONST['n_attention_heads'], # single head attention width * number of heads\n",
    "    'mh_output_width': GLOBAL['embedded_length'], #TODO - I'm just guessing this. Didn't see in illustrated transformer. Since we have to use this for the add & norm layer though it has to be the same as the input width (I think)\n",
    "\n",
    "    # W_q weight matrix \n",
    "    'sh_linear1_input': GLOBAL['embedded_length'], # same as embedded length to end up with n_words x 64\n",
    "    'sh_linear1_output': GLOBAL['continuity_length'], # specified in the paper\n",
    "    # W_k weight matrix \n",
    "    'sh_linear2_input': GLOBAL['embedded_length'], # same as embedded length to end up with n_words x 64\n",
    "    'sh_linear2_output': GLOBAL['continuity_length'], # specified in the paper\n",
    "    # W_v weight matrix \n",
    "    'sh_linear3_input': GLOBAL['embedded_length'], # same as embedded length to end up with n_words x 64\n",
    "    'sh_linear3_output': GLOBAL['continuity_length'], # specified in the paper\n",
    "    \n",
    "    'sh_scale_factor': math.sqrt(GLOBAL['continuity_length']) # specified in the paper, square root of dimension of key vector/matrix (64)\n",
    "}\n",
    "\n",
    "# FeedForward\n",
    "FEEDFORWARD_CONST = {\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEEDFORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_model, dim_ff, dropout=FEEDFORWARD_CONST['dropout']):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim_model, dim_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_ff, dim_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.linear1(x)\n",
    "        x = nn.functional.relu(x) \n",
    "        x = self.dropout(x) \n",
    "        x = self.linear2(x) \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIHEADATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, masked=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.masked = masked\n",
    "        self.attentionHeads = nn.ModuleList([SingleHeadAttention(masked) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(ATTENTION_CONST['mh_concat_width'], ATTENTION_CONST['mh_output_width'])\n",
    "        self.lastHeadKV = None\n",
    "\n",
    "    def forward(self, inputs, encoderKV=None):\n",
    "        x = []\n",
    "        for head in self.attentionHeads:\n",
    "            sh_attention, k, v = head(inputs, encoderKV=encoderKV) \n",
    "            x.append(sh_attention)\n",
    "        self.lastHeadKV = {'K': k,'V': v}\n",
    "        x = torch.cat(x, 1) # concatinate all single head attention outputs\n",
    "        x = self.linear(x) # matmul with weight matrix (linear layer) to get 10x64 shape\n",
    "        return x\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, masked):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.masked = masked\n",
    "        self.linear1 = nn.Linear(ATTENTION_CONST['sh_linear1_input'], ATTENTION_CONST['sh_linear1_output'])\n",
    "        self.linear2 = nn.Linear(ATTENTION_CONST['sh_linear2_input'], ATTENTION_CONST['sh_linear2_output'])\n",
    "        self.linear3 = nn.Linear(ATTENTION_CONST['sh_linear3_input'], ATTENTION_CONST['sh_linear3_output'])\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([ATTENTION_CONST['sh_scale_factor']]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, encoderKV=None):        \n",
    "        q = self.linear1(inputs)\n",
    "        k = self.linear2(inputs) if encoderKV == None else encoderKV['K']\n",
    "        v = self.linear3(inputs) if encoderKV == None else encoderKV['V']\n",
    "        x = torch.matmul(q, k.permute(1, 0)) \n",
    "        x = x * self.scale\n",
    "        # if self.masked:\n",
    "        #     # TODO \"future positions\" have to be set to -inf. this is for the decoder to only allow self attention to consider earlier positions.\n",
    "        x = self.softmax(x) \n",
    "        x = torch.matmul(x, v)\n",
    "        return x if encoderKV != None else x, k, v\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers, n_attention_heads):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoderLayers = nn.ModuleList([DecoderLayer(n_attention_heads) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, inputs, encoderKV):\n",
    "        x = inputs\n",
    "        for layer in self.decoderLayers:\n",
    "            x = layer(x, encoderKV) \n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_attention_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mhattention_masked = MultiHeadAttention(n_attention_heads, masked=True)\n",
    "        self.mhattention = MultiHeadAttention(n_attention_heads)\n",
    "        self.feedforward = FeedForward(DECODER_CONST['ff1'], DECODER_CONST['ff2'])\n",
    "        self.norm1 = nn.LayerNorm(DECODER_CONST['norm1_size'])\n",
    "        self.norm2 = nn.LayerNorm(DECODER_CONST['norm2_size'])\n",
    "        self.norm3 = nn.LayerNorm(DECODER_CONST['norm3_size'])\n",
    "\n",
    "    def forward(self, inputs, encoderKV):\n",
    "        x = inputs\n",
    "        z = x\n",
    "        x = self.mhattention_masked(x) #TODO masking not implemented\n",
    "        x = z + x\n",
    "        x = self.norm1(x)\n",
    "        z = x\n",
    "        x = self.mhattention(x, encoderKV=encoderKV) \n",
    "        x = z + x\n",
    "        x = self.norm2(x)\n",
    "        z = x\n",
    "        x = self.feedforward(x)\n",
    "        x = z + x\n",
    "        x = self.norm3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layer, n_attention_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoderLayers = nn.ModuleList([EncoderLayer(n_attention_heads) for _ in range(n_layer)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        lastLayerKV = None\n",
    "        for layer in self.encoderLayers:\n",
    "            x = layer(x)\n",
    "            lastLayerKV = layer.lastLayerKV\n",
    "        return x, lastLayerKV\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_attention_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mhattention = MultiHeadAttention(n_attention_heads)\n",
    "        self.feedforward = FeedForward(ENCODER_CONST['ff1'], ENCODER_CONST['ff2'])\n",
    "        self.norm1 = nn.LayerNorm(ENCODER_CONST['norm1_size'])\n",
    "        self.norm2 = nn.LayerNorm(ENCODER_CONST['norm2_size'])\n",
    "        self.lastLayerKV = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs \n",
    "        z = x\n",
    "        x = self.mhattention(x)\n",
    "        self.lastLayerKV = self.mhattention.lastHeadKV\n",
    "        x = z + x\n",
    "        x = self.norm1(x)\n",
    "        z = x\n",
    "        x = self.feedforward(x)\n",
    "        x = z + x\n",
    "        x = self.norm2(x) \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_layers=TRANS_CONST['n_attention_layers'], n_attention_heads=TRANS_CONST['n_attention_heads']):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(n_layers, n_attention_heads)\n",
    "        self.decoder = Decoder(n_layers, n_attention_heads)\n",
    "        self.embedding = nn.Embedding(TRANS_CONST['embedding_dic_size'], TRANS_CONST['embedded_vec_size'])\n",
    "        # self.posEncoding = #TODO\n",
    "        self.linear = nn.Linear(TRANS_CONST['linear_input'], TRANS_CONST['linear_output'])\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def __call__(self, inputs=None):\n",
    "        if inputs != None: \n",
    "            raise NotImplementedError\n",
    "\n",
    "        import random\n",
    "        inputs = []\n",
    "        for _ in range(13): inputs.append(numpy.zeros(26)) # 26 is vocab size, should be constant; 13 is just a random amount of words in the sequence\n",
    "        inputs = torch.Tensor(inputs)\n",
    "        for i in inputs: i[random.randint(0, len(i) - 1)] = 1\n",
    "        return self.forward(inputs.long())\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # TODO FIRST PRIO\n",
    "            # get all inputs for embedding (real example for translation tasks etc, noise, decoder input) on the same format, which should be NxV\n",
    "        #### ENCODING ####\n",
    "        x = self.doEmbedding(inputs)\n",
    "        # x = self.posEncoding(x)\n",
    "        _, encoderKV = self.encoder(x) #TODO try running the encoder output trough 2 additional linear layers to make the KV matrices\n",
    "\n",
    "        #### DECODING ####\n",
    "        sos = numpy.zeros(GLOBAL['n_vocab'])\n",
    "        sos[0] = 1\n",
    "        x = torch.Tensor([sos])\n",
    "        ## Embedding\n",
    "        x = self.doEmbedding(x)\n",
    "        # x = self.posEncoding(x) #TODO\n",
    "        ## Decoding\n",
    "        while len(x) < TRANS_CONST['max_output_length']: #TODO add eos token\n",
    "            new_word = self.decoder(x, encoderKV)[0, :].unsqueeze(dim=0)\n",
    "            x = torch.cat([x, new_word], dim=0)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def doEmbedding(self, inputs):\n",
    "        inputs = inputs.nonzero()[:, 1] # this gets all indices of nonzero values from the inputs matrix\n",
    "        return self.embedding(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1707, grad_fn=<BinaryCrossEntropyBackward>) tensor([[0.0118, 0.0626, 0.0102, 0.0486, 0.0378, 0.0515, 0.0258, 0.0468, 0.0447,\n",
      "         0.0118, 0.0204, 0.0265, 0.0610, 0.0135, 0.0146, 0.0135, 0.0378, 0.0340,\n",
      "         0.0438, 0.0410, 0.0609, 0.0580, 0.0265, 0.0343, 0.0340, 0.0637, 0.0295,\n",
      "         0.0208, 0.0150],\n",
      "        [0.0147, 0.0584, 0.0134, 0.0416, 0.0584, 0.0568, 0.0355, 0.0312, 0.0374,\n",
      "         0.0214, 0.0105, 0.0078, 0.0984, 0.0256, 0.0185, 0.0169, 0.0394, 0.0255,\n",
      "         0.0287, 0.0524, 0.0221, 0.0641, 0.0141, 0.0365, 0.0066, 0.0768, 0.0394,\n",
      "         0.0199, 0.0281],\n",
      "        [0.0232, 0.0434, 0.0179, 0.0626, 0.0617, 0.0347, 0.0377, 0.0201, 0.0474,\n",
      "         0.0113, 0.0084, 0.0142, 0.0807, 0.0176, 0.0503, 0.0288, 0.0436, 0.0214,\n",
      "         0.0144, 0.0286, 0.0391, 0.0300, 0.0300, 0.0316, 0.0098, 0.0641, 0.0597,\n",
      "         0.0193, 0.0485],\n",
      "        [0.0254, 0.0640, 0.0158, 0.0486, 0.0885, 0.0431, 0.0307, 0.0180, 0.0580,\n",
      "         0.0230, 0.0179, 0.0110, 0.0939, 0.0125, 0.0282, 0.0294, 0.0305, 0.0381,\n",
      "         0.0193, 0.0179, 0.0308, 0.0290, 0.0267, 0.0277, 0.0177, 0.0537, 0.0411,\n",
      "         0.0325, 0.0270],\n",
      "        [0.0232, 0.0583, 0.0327, 0.0717, 0.0748, 0.0222, 0.0281, 0.0261, 0.0197,\n",
      "         0.0218, 0.0122, 0.0092, 0.0644, 0.0161, 0.0232, 0.0440, 0.0194, 0.0244,\n",
      "         0.0282, 0.0283, 0.0241, 0.0428, 0.0226, 0.0344, 0.0151, 0.1123, 0.0281,\n",
      "         0.0299, 0.0428],\n",
      "        [0.0148, 0.0782, 0.0214, 0.0912, 0.0725, 0.0247, 0.0268, 0.0341, 0.0280,\n",
      "         0.0303, 0.0094, 0.0138, 0.0500, 0.0165, 0.0221, 0.0265, 0.0213, 0.0395,\n",
      "         0.0141, 0.0362, 0.0246, 0.0502, 0.0247, 0.0353, 0.0106, 0.0953, 0.0370,\n",
      "         0.0250, 0.0261]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.1102, grad_fn=<BinaryCrossEntropyBackward>) tensor([[0.0360, 0.0549, 0.0101, 0.0409, 0.0635, 0.0406, 0.0216, 0.0782, 0.0386,\n",
      "         0.0116, 0.0203, 0.0473, 0.0538, 0.0135, 0.0259, 0.0129, 0.0310, 0.0295,\n",
      "         0.0383, 0.0334, 0.0555, 0.0439, 0.0242, 0.0266, 0.0298, 0.0552, 0.0296,\n",
      "         0.0192, 0.0143],\n",
      "        [0.0302, 0.0213, 0.0091, 0.0129, 0.0952, 0.0094, 0.0119, 0.0611, 0.0307,\n",
      "         0.0287, 0.0177, 0.2714, 0.0192, 0.0286, 0.0701, 0.0153, 0.0109, 0.0268,\n",
      "         0.0191, 0.0125, 0.0246, 0.0054, 0.0114, 0.0058, 0.0416, 0.0400, 0.0336,\n",
      "         0.0123, 0.0232],\n",
      "        [0.0311, 0.0215, 0.0096, 0.0125, 0.0956, 0.0091, 0.0123, 0.0578, 0.0311,\n",
      "         0.0266, 0.0182, 0.2842, 0.0172, 0.0261, 0.0687, 0.0162, 0.0108, 0.0259,\n",
      "         0.0198, 0.0111, 0.0234, 0.0050, 0.0108, 0.0068, 0.0402, 0.0378, 0.0344,\n",
      "         0.0119, 0.0243],\n",
      "        [0.0312, 0.0228, 0.0099, 0.0130, 0.0938, 0.0094, 0.0116, 0.0561, 0.0299,\n",
      "         0.0295, 0.0183, 0.2644, 0.0176, 0.0302, 0.0725, 0.0144, 0.0114, 0.0274,\n",
      "         0.0206, 0.0126, 0.0248, 0.0050, 0.0109, 0.0066, 0.0415, 0.0382, 0.0382,\n",
      "         0.0133, 0.0248],\n",
      "        [0.0294, 0.0232, 0.0084, 0.0130, 0.0935, 0.0098, 0.0117, 0.0660, 0.0319,\n",
      "         0.0268, 0.0201, 0.2881, 0.0181, 0.0252, 0.0658, 0.0149, 0.0101, 0.0262,\n",
      "         0.0188, 0.0121, 0.0221, 0.0050, 0.0099, 0.0058, 0.0372, 0.0380, 0.0340,\n",
      "         0.0121, 0.0230],\n",
      "        [0.0299, 0.0227, 0.0092, 0.0127, 0.0941, 0.0096, 0.0132, 0.0629, 0.0305,\n",
      "         0.0240, 0.0186, 0.2828, 0.0171, 0.0279, 0.0663, 0.0150, 0.0095, 0.0261,\n",
      "         0.0202, 0.0121, 0.0238, 0.0049, 0.0116, 0.0064, 0.0423, 0.0369, 0.0344,\n",
      "         0.0120, 0.0234]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.0846, grad_fn=<BinaryCrossEntropyBackward>) tensor([[0.1049, 0.0442, 0.0093, 0.0297, 0.0816, 0.0269, 0.0169, 0.0996, 0.0336,\n",
      "         0.0117, 0.0197, 0.0665, 0.0421, 0.0135, 0.0349, 0.0118, 0.0231, 0.0255,\n",
      "         0.0322, 0.0240, 0.0486, 0.0267, 0.0209, 0.0170, 0.0283, 0.0466, 0.0290,\n",
      "         0.0172, 0.0137],\n",
      "        [0.0054, 0.0021, 0.0009, 0.0015, 0.1544, 0.0014, 0.0017, 0.1258, 0.0042,\n",
      "         0.0035, 0.0022, 0.5125, 0.0016, 0.0026, 0.1497, 0.0013, 0.0012, 0.0031,\n",
      "         0.0026, 0.0023, 0.0014, 0.0007, 0.0012, 0.0013, 0.0051, 0.0045, 0.0018,\n",
      "         0.0011, 0.0028],\n",
      "        [0.0053, 0.0021, 0.0010, 0.0016, 0.1501, 0.0014, 0.0017, 0.1321, 0.0040,\n",
      "         0.0035, 0.0023, 0.5134, 0.0015, 0.0025, 0.1462, 0.0014, 0.0012, 0.0031,\n",
      "         0.0025, 0.0022, 0.0014, 0.0007, 0.0012, 0.0014, 0.0054, 0.0048, 0.0020,\n",
      "         0.0011, 0.0028],\n",
      "        [0.0056, 0.0021, 0.0009, 0.0015, 0.1565, 0.0013, 0.0018, 0.1267, 0.0044,\n",
      "         0.0035, 0.0022, 0.5091, 0.0015, 0.0026, 0.1500, 0.0014, 0.0012, 0.0031,\n",
      "         0.0026, 0.0023, 0.0013, 0.0007, 0.0013, 0.0013, 0.0050, 0.0045, 0.0019,\n",
      "         0.0011, 0.0027],\n",
      "        [0.0059, 0.0024, 0.0011, 0.0015, 0.1460, 0.0015, 0.0020, 0.1326, 0.0047,\n",
      "         0.0036, 0.0023, 0.5115, 0.0017, 0.0028, 0.1479, 0.0014, 0.0012, 0.0033,\n",
      "         0.0025, 0.0025, 0.0015, 0.0008, 0.0014, 0.0014, 0.0053, 0.0048, 0.0021,\n",
      "         0.0013, 0.0029],\n",
      "        [0.0051, 0.0020, 0.0009, 0.0015, 0.1430, 0.0013, 0.0018, 0.1255, 0.0038,\n",
      "         0.0031, 0.0022, 0.5284, 0.0013, 0.0025, 0.1481, 0.0013, 0.0011, 0.0028,\n",
      "         0.0023, 0.0022, 0.0014, 0.0006, 0.0011, 0.0013, 0.0052, 0.0044, 0.0018,\n",
      "         0.0011, 0.0027]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.0856, grad_fn=<BinaryCrossEntropyBackward>) tensor([[0.2917, 0.0278, 0.0073, 0.0168, 0.0828, 0.0140, 0.0108, 0.1005, 0.0240,\n",
      "         0.0103, 0.0158, 0.0769, 0.0254, 0.0116, 0.0379, 0.0091, 0.0134, 0.0181,\n",
      "         0.0213, 0.0136, 0.0329, 0.0125, 0.0143, 0.0086, 0.0228, 0.0322, 0.0238,\n",
      "         0.0125, 0.0113],\n",
      "        [0.0011, 0.0004, 0.0002, 0.0002, 0.2855, 0.0002, 0.0003, 0.3284, 0.0006,\n",
      "         0.0004, 0.0003, 0.0963, 0.0002, 0.0003, 0.2811, 0.0002, 0.0002, 0.0005,\n",
      "         0.0004, 0.0005, 0.0001, 0.0001, 0.0002, 0.0003, 0.0006, 0.0007, 0.0002,\n",
      "         0.0002, 0.0003],\n",
      "        [0.0009, 0.0003, 0.0001, 0.0002, 0.3079, 0.0002, 0.0002, 0.3069, 0.0005,\n",
      "         0.0004, 0.0003, 0.0877, 0.0002, 0.0003, 0.2901, 0.0002, 0.0002, 0.0004,\n",
      "         0.0003, 0.0004, 0.0001, 0.0001, 0.0002, 0.0003, 0.0005, 0.0006, 0.0001,\n",
      "         0.0001, 0.0003],\n",
      "        [0.0009, 0.0003, 0.0001, 0.0002, 0.3091, 0.0002, 0.0003, 0.2926, 0.0005,\n",
      "         0.0004, 0.0003, 0.0904, 0.0002, 0.0003, 0.3000, 0.0002, 0.0002, 0.0004,\n",
      "         0.0003, 0.0005, 0.0001, 0.0001, 0.0002, 0.0003, 0.0005, 0.0006, 0.0002,\n",
      "         0.0001, 0.0003],\n",
      "        [0.0010, 0.0003, 0.0002, 0.0002, 0.3082, 0.0002, 0.0003, 0.3231, 0.0005,\n",
      "         0.0004, 0.0003, 0.0898, 0.0002, 0.0003, 0.2709, 0.0002, 0.0002, 0.0004,\n",
      "         0.0003, 0.0004, 0.0001, 0.0001, 0.0002, 0.0002, 0.0005, 0.0006, 0.0002,\n",
      "         0.0001, 0.0003],\n",
      "        [0.0009, 0.0003, 0.0001, 0.0002, 0.3006, 0.0002, 0.0002, 0.3189, 0.0005,\n",
      "         0.0004, 0.0003, 0.0839, 0.0002, 0.0002, 0.2893, 0.0002, 0.0002, 0.0004,\n",
      "         0.0003, 0.0004, 0.0001, 0.0001, 0.0002, 0.0003, 0.0005, 0.0005, 0.0001,\n",
      "         0.0001, 0.0003]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.0733, grad_fn=<BinaryCrossEntropyBackward>) tensor([[0.6205, 0.0110, 0.0037, 0.0061, 0.0512, 0.0047, 0.0044, 0.0620, 0.0108,\n",
      "         0.0061, 0.0083, 0.0583, 0.0097, 0.0067, 0.0276, 0.0047, 0.0049, 0.0081,\n",
      "         0.0088, 0.0050, 0.0138, 0.0039, 0.0061, 0.0028, 0.0118, 0.0142, 0.0129,\n",
      "         0.0057, 0.0062],\n",
      "        [0.0004, 0.0001, 0.0000, 0.0001, 0.2684, 0.0001, 0.0001, 0.2745, 0.0001,\n",
      "         0.0001, 0.0001, 0.1672, 0.0001, 0.0001, 0.2877, 0.0000, 0.0001, 0.0001,\n",
      "         0.0001, 0.0001, 0.0000, 0.0000, 0.0001, 0.0001, 0.0001, 0.0001, 0.0000,\n",
      "         0.0000, 0.0001],\n",
      "        [0.0003, 0.0001, 0.0000, 0.0001, 0.2530, 0.0001, 0.0001, 0.2828, 0.0001,\n",
      "         0.0001, 0.0001, 0.1665, 0.0001, 0.0001, 0.2955, 0.0000, 0.0001, 0.0001,\n",
      "         0.0001, 0.0001, 0.0000, 0.0000, 0.0001, 0.0001, 0.0001, 0.0001, 0.0000,\n",
      "         0.0000, 0.0001],\n",
      "        [0.0004, 0.0001, 0.0000, 0.0001, 0.2628, 0.0001, 0.0001, 0.2836, 0.0001,\n",
      "         0.0001, 0.0001, 0.1720, 0.0001, 0.0001, 0.2794, 0.0000, 0.0001, 0.0001,\n",
      "         0.0001, 0.0002, 0.0000, 0.0000, 0.0001, 0.0001, 0.0001, 0.0001, 0.0000,\n",
      "         0.0000, 0.0001],\n",
      "        [0.0003, 0.0001, 0.0000, 0.0001, 0.2548, 0.0001, 0.0001, 0.2940, 0.0001,\n",
      "         0.0001, 0.0001, 0.1695, 0.0001, 0.0001, 0.2796, 0.0000, 0.0001, 0.0001,\n",
      "         0.0001, 0.0001, 0.0000, 0.0000, 0.0001, 0.0001, 0.0001, 0.0001, 0.0000,\n",
      "         0.0000, 0.0001],\n",
      "        [0.0003, 0.0001, 0.0000, 0.0001, 0.2405, 0.0001, 0.0001, 0.2866, 0.0001,\n",
      "         0.0001, 0.0001, 0.1790, 0.0001, 0.0001, 0.2919, 0.0000, 0.0001, 0.0001,\n",
      "         0.0001, 0.0001, 0.0000, 0.0000, 0.0001, 0.0001, 0.0001, 0.0001, 0.0000,\n",
      "         0.0000, 0.0001]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0873, grad_fn=<BinaryCrossEntropyBackward>) tensor([[8.7451e-01, 2.5409e-03, 1.1243e-03, 1.3373e-03, 1.7430e-02, 9.6225e-04,\n",
      "         1.0473e-03, 2.1078e-02, 2.8653e-03, 2.1577e-03, 2.5629e-03, 2.7009e-02,\n",
      "         2.1927e-03, 2.2779e-03, 1.2005e-02, 1.4055e-03, 1.0650e-03, 2.1383e-03,\n",
      "         2.1420e-03, 1.0852e-03, 3.3577e-03, 7.3908e-04, 1.5342e-03, 5.6726e-04,\n",
      "         3.5073e-03, 3.6539e-03, 4.1558e-03, 1.5381e-03, 2.0094e-03],\n",
      "        [1.1812e-04, 2.1182e-05, 8.9074e-06, 1.7073e-05, 6.0815e-02, 1.7166e-05,\n",
      "         1.9178e-05, 5.0153e-02, 2.3949e-05, 1.8548e-05, 2.0692e-05, 8.2141e-01,\n",
      "         1.4403e-05, 1.0579e-05, 6.7086e-02, 9.5734e-06, 1.5336e-05, 2.2610e-05,\n",
      "         2.6098e-05, 3.4454e-05, 7.1902e-06, 1.2554e-05, 1.3592e-05, 2.3386e-05,\n",
      "         2.6472e-05, 2.9304e-05, 7.5860e-06, 6.4949e-06, 1.3946e-05],\n",
      "        [1.1652e-04, 2.0946e-05, 8.5940e-06, 1.6992e-05, 5.9126e-02, 1.7461e-05,\n",
      "         1.8885e-05, 4.3832e-02, 2.4602e-05, 1.9038e-05, 1.9929e-05, 8.2686e-01,\n",
      "         1.4353e-05, 1.1092e-05, 6.9647e-02, 9.7411e-06, 1.4673e-05, 2.1809e-05,\n",
      "         2.6604e-05, 3.2556e-05, 7.3361e-06, 1.2196e-05, 1.3109e-05, 2.3464e-05,\n",
      "         2.5569e-05, 3.0810e-05, 7.4917e-06, 6.1824e-06, 1.3354e-05],\n",
      "        [1.0206e-04, 1.7871e-05, 7.9978e-06, 1.5681e-05, 5.3596e-02, 1.5442e-05,\n",
      "         1.7535e-05, 3.9632e-02, 2.2935e-05, 1.7257e-05, 1.9099e-05, 8.3838e-01,\n",
      "         1.2865e-05, 9.9183e-06, 6.7909e-02, 8.5652e-06, 1.3527e-05, 2.0056e-05,\n",
      "         2.3940e-05, 3.0291e-05, 6.5654e-06, 1.0311e-05, 1.2676e-05, 2.2537e-05,\n",
      "         2.4317e-05, 2.5621e-05, 6.7108e-06, 5.9426e-06, 1.3112e-05],\n",
      "        [1.0382e-04, 1.9991e-05, 8.6091e-06, 1.6448e-05, 5.7156e-02, 1.6214e-05,\n",
      "         1.8366e-05, 4.4292e-02, 2.3851e-05, 1.7860e-05, 1.9460e-05, 8.3246e-01,\n",
      "         1.4235e-05, 1.0135e-05, 6.5594e-02, 9.3427e-06, 1.4238e-05, 2.0714e-05,\n",
      "         2.4951e-05, 3.2028e-05, 6.6762e-06, 1.1570e-05, 1.2792e-05, 2.2520e-05,\n",
      "         2.4595e-05, 2.7551e-05, 6.7253e-06, 6.0850e-06, 1.3331e-05],\n",
      "        [1.0499e-04, 1.8429e-05, 8.0762e-06, 1.5958e-05, 5.7343e-02, 1.5688e-05,\n",
      "         1.7645e-05, 4.4540e-02, 2.2917e-05, 1.7807e-05, 1.8856e-05, 8.3517e-01,\n",
      "         1.3632e-05, 1.0020e-05, 6.2458e-02, 9.0149e-06, 1.4113e-05, 2.0719e-05,\n",
      "         2.4047e-05, 3.1588e-05, 6.5407e-06, 1.0893e-05, 1.2778e-05, 2.2373e-05,\n",
      "         2.3862e-05, 2.6401e-05, 6.8356e-06, 5.7534e-06, 1.2830e-05]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.0892, grad_fn=<BinaryCrossEntropyBackward>) tensor([[9.6713e-01, 4.4974e-04, 2.5642e-04, 2.2665e-04, 4.5842e-03, 1.5309e-04,\n",
      "         1.9076e-04, 5.2821e-03, 5.7241e-04, 5.7583e-04, 5.9568e-04, 9.0970e-03,\n",
      "         3.8324e-04, 5.8793e-04, 4.0017e-03, 3.1911e-04, 1.7952e-04, 4.2452e-04,\n",
      "         3.9361e-04, 1.8334e-04, 6.2009e-04, 1.1075e-04, 2.9088e-04, 8.8886e-05,\n",
      "         7.7616e-04, 7.1342e-04, 1.0122e-03, 3.1028e-04, 4.8830e-04],\n",
      "        [7.3986e-05, 9.8660e-06, 3.4484e-06, 7.0624e-06, 6.9986e-02, 7.8514e-06,\n",
      "         7.3383e-06, 2.7166e-02, 8.5832e-06, 6.4004e-06, 7.6853e-06, 8.3062e-01,\n",
      "         6.2652e-06, 4.0580e-06, 7.1985e-02, 3.5608e-06, 6.6128e-06, 7.7261e-06,\n",
      "         1.0526e-05, 1.4174e-05, 3.0421e-06, 5.9611e-06, 5.3881e-06, 1.0773e-05,\n",
      "         9.1271e-06, 1.1560e-05, 2.7778e-06, 2.3956e-06, 5.1416e-06],\n",
      "        [7.0325e-05, 8.9969e-06, 3.2829e-06, 7.1415e-06, 6.4452e-02, 7.7652e-06,\n",
      "         6.9912e-06, 2.7133e-02, 8.4880e-06, 6.2454e-06, 7.6279e-06, 8.3918e-01,\n",
      "         5.9860e-06, 3.8114e-06, 6.8999e-02, 3.4326e-06, 6.4422e-06, 7.3285e-06,\n",
      "         1.0649e-05, 1.3505e-05, 2.9202e-06, 5.6955e-06, 5.2045e-06, 1.0674e-05,\n",
      "         8.7799e-06, 1.0914e-05, 2.7094e-06, 2.3437e-06, 4.9222e-06],\n",
      "        [7.9566e-05, 9.9844e-06, 3.5514e-06, 7.0223e-06, 6.9981e-02, 8.0790e-06,\n",
      "         7.3806e-06, 2.8855e-02, 9.0039e-06, 6.3612e-06, 7.9165e-06, 8.3224e-01,\n",
      "         6.6345e-06, 4.3233e-06, 6.8679e-02, 3.5483e-06, 6.6967e-06, 8.0478e-06,\n",
      "         1.0728e-05, 1.4313e-05, 3.1653e-06, 6.0706e-06, 5.3049e-06, 1.0540e-05,\n",
      "         8.9249e-06, 1.1306e-05, 2.8420e-06, 2.4731e-06, 5.0957e-06],\n",
      "        [7.0413e-05, 9.6126e-06, 3.3539e-06, 6.9068e-06, 6.4411e-02, 8.0903e-06,\n",
      "         7.0767e-06, 2.7187e-02, 8.8069e-06, 6.2238e-06, 7.6329e-06, 8.3720e-01,\n",
      "         6.2271e-06, 3.9899e-06, 7.0969e-02, 3.5235e-06, 6.6226e-06, 7.6172e-06,\n",
      "         1.0431e-05, 1.3748e-05, 3.0083e-06, 6.0933e-06, 5.3708e-06, 1.0453e-05,\n",
      "         9.1376e-06, 1.1135e-05, 2.8940e-06, 2.4478e-06, 4.9927e-06],\n",
      "        [7.6450e-05, 1.0252e-05, 3.5441e-06, 7.3026e-06, 6.9783e-02, 8.0126e-06,\n",
      "         7.6637e-06, 2.7655e-02, 8.7596e-06, 6.8255e-06, 8.0639e-06, 8.3296e-01,\n",
      "         6.4382e-06, 4.3073e-06, 6.9347e-02, 3.7706e-06, 6.7542e-06, 8.1105e-06,\n",
      "         1.1111e-05, 1.4468e-05, 3.1490e-06, 6.2799e-06, 5.5839e-06, 1.0856e-05,\n",
      "         9.4674e-06, 1.1898e-05, 2.8983e-06, 2.4935e-06, 5.1923e-06]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.0756, grad_fn=<BinaryCrossEntropyBackward>) tensor([[9.9109e-01, 7.5961e-05, 5.5003e-05, 3.6818e-05, 1.3519e-03, 2.3428e-05,\n",
      "         3.3033e-05, 1.3863e-03, 1.0837e-04, 1.4411e-04, 1.3016e-04, 2.8311e-03,\n",
      "         6.4080e-05, 1.4262e-04, 1.3631e-03, 6.8248e-05, 2.9026e-05, 7.9880e-05,\n",
      "         6.8736e-05, 2.9685e-05, 1.0881e-04, 1.6020e-05, 5.2293e-05, 1.3398e-05,\n",
      "         1.6136e-04, 1.3205e-04, 2.3240e-04, 5.9125e-05, 1.1130e-04],\n",
      "        [1.1339e-04, 9.7776e-06, 2.5556e-06, 5.2610e-06, 4.1534e-01, 7.4641e-06,\n",
      "         4.5323e-06, 1.0493e-01, 5.7641e-06, 4.0457e-06, 5.0932e-06, 1.2182e-01,\n",
      "         5.7404e-06, 3.3010e-06, 3.5766e-01, 2.4738e-06, 4.9947e-06, 4.8221e-06,\n",
      "         6.9619e-06, 1.1381e-05, 2.4321e-06, 6.6274e-06, 3.5382e-06, 8.6829e-06,\n",
      "         5.1695e-06, 9.6166e-06, 2.1866e-06, 2.0939e-06, 3.2851e-06],\n",
      "        [1.0531e-04, 8.9116e-06, 2.4488e-06, 4.9601e-06, 4.1112e-01, 6.5197e-06,\n",
      "         4.4530e-06, 1.0045e-01, 5.3130e-06, 3.9200e-06, 4.6884e-06, 1.1958e-01,\n",
      "         5.2781e-06, 2.9419e-06, 3.6862e-01, 2.3027e-06, 4.6992e-06, 4.7422e-06,\n",
      "         6.4122e-06, 1.0982e-05, 2.3306e-06, 5.9752e-06, 3.4773e-06, 8.4006e-06,\n",
      "         4.9052e-06, 8.9741e-06, 2.0033e-06, 1.9421e-06, 3.1099e-06],\n",
      "        [1.1274e-04, 9.8104e-06, 2.6901e-06, 5.3825e-06, 4.0895e-01, 7.4502e-06,\n",
      "         4.8319e-06, 1.0025e-01, 5.7854e-06, 4.0888e-06, 5.0004e-06, 1.1988e-01,\n",
      "         5.7140e-06, 3.3205e-06, 3.7068e-01, 2.5024e-06, 5.0366e-06, 4.9924e-06,\n",
      "         6.8258e-06, 1.1718e-05, 2.4878e-06, 6.4228e-06, 3.5706e-06, 8.9359e-06,\n",
      "         5.2416e-06, 9.5373e-06, 2.1605e-06, 2.1332e-06, 3.2824e-06],\n",
      "        [1.0579e-04, 9.4232e-06, 2.4997e-06, 5.0477e-06, 4.0496e-01, 7.0448e-06,\n",
      "         4.5674e-06, 1.0266e-01, 5.6902e-06, 3.9034e-06, 4.7843e-06, 1.2413e-01,\n",
      "         5.5520e-06, 3.1252e-06, 3.6803e-01, 2.3654e-06, 5.0028e-06, 4.8521e-06,\n",
      "         6.6231e-06, 1.1098e-05, 2.4478e-06, 6.3245e-06, 3.3782e-06, 8.6394e-06,\n",
      "         4.9915e-06, 8.8065e-06, 2.0722e-06, 2.0367e-06, 3.2173e-06],\n",
      "        [1.0969e-04, 9.6690e-06, 2.6538e-06, 5.2904e-06, 4.1362e-01, 7.2918e-06,\n",
      "         4.7214e-06, 1.0936e-01, 5.7049e-06, 4.0651e-06, 5.1425e-06, 1.1773e-01,\n",
      "         5.6889e-06, 3.2260e-06, 3.5905e-01, 2.4273e-06, 4.8530e-06, 5.0673e-06,\n",
      "         6.5667e-06, 1.1433e-05, 2.5594e-06, 6.4008e-06, 3.4632e-06, 9.0382e-06,\n",
      "         5.1835e-06, 9.3612e-06, 2.1004e-06, 2.1283e-06, 3.3675e-06]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.1084, grad_fn=<BinaryCrossEntropyBackward>) tensor([[9.9738e-01, 1.3206e-05, 1.2030e-05, 6.1617e-06, 4.1726e-04, 3.7004e-06,\n",
      "         5.8754e-06, 4.1119e-04, 2.1056e-05, 3.6683e-05, 2.9011e-05, 9.0033e-04,\n",
      "         1.1032e-05, 3.5248e-05, 4.8095e-04, 1.4901e-05, 4.8416e-06, 1.5425e-05,\n",
      "         1.2336e-05, 4.9550e-06, 1.9617e-05, 2.3940e-06, 9.6459e-06, 2.0837e-06,\n",
      "         3.4279e-05, 2.5078e-05, 5.4470e-05, 1.1535e-05, 2.5831e-05],\n",
      "        [5.5699e-05, 3.0250e-06, 6.5339e-07, 1.3999e-06, 4.5335e-01, 2.2974e-06,\n",
      "         1.0580e-06, 1.6568e-01, 1.3735e-06, 8.5013e-07, 1.0617e-06, 5.9789e-03,\n",
      "         1.7840e-06, 8.9527e-07, 3.7490e-01, 5.6705e-07, 1.3595e-06, 1.1711e-06,\n",
      "         1.5802e-06, 3.0115e-06, 7.6277e-07, 2.4962e-06, 7.7992e-07, 2.5374e-06,\n",
      "         1.0261e-06, 2.4101e-06, 5.5686e-07, 6.3722e-07, 7.3741e-07],\n",
      "        [5.5116e-05, 2.9885e-06, 6.2727e-07, 1.3566e-06, 4.4095e-01, 2.2696e-06,\n",
      "         1.0655e-06, 1.6038e-01, 1.3191e-06, 8.5641e-07, 1.0454e-06, 6.0127e-03,\n",
      "         1.7769e-06, 8.6017e-07, 3.9257e-01, 5.5475e-07, 1.3093e-06, 1.1283e-06,\n",
      "         1.5254e-06, 3.0020e-06, 7.3573e-07, 2.4262e-06, 7.7661e-07, 2.4809e-06,\n",
      "         1.0244e-06, 2.3535e-06, 5.6222e-07, 6.2507e-07, 7.2716e-07],\n",
      "        [5.6124e-05, 2.9710e-06, 6.5468e-07, 1.4003e-06, 4.4086e-01, 2.2848e-06,\n",
      "         1.0525e-06, 1.6718e-01, 1.3236e-06, 8.5586e-07, 1.0501e-06, 5.8139e-03,\n",
      "         1.7831e-06, 8.5644e-07, 3.8606e-01, 5.6368e-07, 1.3509e-06, 1.1581e-06,\n",
      "         1.5174e-06, 3.0460e-06, 7.4596e-07, 2.4509e-06, 7.7378e-07, 2.5093e-06,\n",
      "         1.0258e-06, 2.4140e-06, 5.6206e-07, 6.2942e-07, 7.3341e-07],\n",
      "        [5.9236e-05, 3.0245e-06, 6.4584e-07, 1.4089e-06, 4.3970e-01, 2.2875e-06,\n",
      "         1.0773e-06, 1.6209e-01, 1.3419e-06, 8.8142e-07, 1.0641e-06, 6.1565e-03,\n",
      "         1.8021e-06, 9.0422e-07, 3.9197e-01, 5.8182e-07, 1.3463e-06, 1.1782e-06,\n",
      "         1.6196e-06, 3.0835e-06, 7.6846e-07, 2.4697e-06, 8.1095e-07, 2.4723e-06,\n",
      "         1.0376e-06, 2.4405e-06, 5.8931e-07, 6.3674e-07, 7.6442e-07],\n",
      "        [6.0175e-05, 3.1296e-06, 6.7191e-07, 1.4282e-06, 4.4165e-01, 2.4241e-06,\n",
      "         1.1220e-06, 1.6907e-01, 1.4062e-06, 8.9389e-07, 1.1140e-06, 6.1094e-03,\n",
      "         1.8578e-06, 9.0898e-07, 3.8308e-01, 5.9442e-07, 1.3836e-06, 1.2349e-06,\n",
      "         1.6075e-06, 3.1809e-06, 8.0853e-07, 2.6088e-06, 8.3379e-07, 2.5388e-06,\n",
      "         1.0716e-06, 2.4432e-06, 6.0775e-07, 6.6537e-07, 7.8808e-07]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1217, grad_fn=<BinaryCrossEntropyBackward>) tensor([[9.9919e-01, 2.3933e-06, 2.7270e-06, 1.0744e-06, 1.2154e-04, 6.0953e-07,\n",
      "         1.0880e-06, 1.3307e-04, 4.2593e-06, 9.6625e-06, 6.7053e-06, 2.9415e-04,\n",
      "         1.9789e-06, 9.0255e-06, 1.7155e-04, 3.3746e-06, 8.4258e-07, 3.1012e-06,\n",
      "         2.3063e-06, 8.6245e-07, 3.6835e-06, 3.7309e-07, 1.8516e-06, 3.3783e-07,\n",
      "         7.5678e-06, 4.9567e-06, 1.3242e-05, 2.3389e-06, 6.2084e-06],\n",
      "        [5.1319e-05, 1.6064e-06, 2.7838e-07, 6.5169e-07, 2.6068e-01, 1.2995e-06,\n",
      "         4.6008e-07, 4.0768e-01, 5.6755e-07, 3.3657e-07, 4.0277e-07, 1.7061e-03,\n",
      "         9.8713e-07, 4.0920e-07, 3.2986e-01, 2.4079e-07, 6.4926e-07, 5.0148e-07,\n",
      "         7.1880e-07, 1.4080e-06, 4.2871e-07, 1.7273e-06, 3.3349e-07, 1.3085e-06,\n",
      "         3.8778e-07, 1.0576e-06, 2.9067e-07, 3.1824e-07, 3.0290e-07],\n",
      "        [5.1148e-05, 1.5834e-06, 2.8208e-07, 6.4680e-07, 2.5168e-01, 1.3194e-06,\n",
      "         4.6224e-07, 4.2230e-01, 5.5749e-07, 3.3640e-07, 4.0999e-07, 1.6103e-03,\n",
      "         9.7568e-07, 4.0917e-07, 3.2434e-01, 2.4291e-07, 6.3562e-07, 5.0657e-07,\n",
      "         7.1868e-07, 1.4399e-06, 4.2289e-07, 1.7385e-06, 3.4040e-07, 1.3112e-06,\n",
      "         3.8784e-07, 1.0865e-06, 2.8360e-07, 3.0769e-07, 3.0451e-07],\n",
      "        [4.6757e-05, 1.4812e-06, 2.7033e-07, 6.2879e-07, 2.5365e-01, 1.2082e-06,\n",
      "         4.3388e-07, 4.0631e-01, 5.3313e-07, 3.1897e-07, 3.7996e-07, 1.5262e-03,\n",
      "         8.9742e-07, 3.7950e-07, 3.3845e-01, 2.3192e-07, 6.0867e-07, 4.6753e-07,\n",
      "         6.6396e-07, 1.3635e-06, 3.8722e-07, 1.5777e-06, 3.1750e-07, 1.2231e-06,\n",
      "         3.6005e-07, 9.8852e-07, 2.6768e-07, 2.8942e-07, 2.8541e-07],\n",
      "        [5.1821e-05, 1.6412e-06, 2.8576e-07, 6.7594e-07, 2.6315e-01, 1.3850e-06,\n",
      "         4.6865e-07, 4.0747e-01, 5.9082e-07, 3.4277e-07, 4.0084e-07, 1.6765e-03,\n",
      "         1.0069e-06, 4.2044e-07, 3.2763e-01, 2.4729e-07, 6.7126e-07, 5.2262e-07,\n",
      "         7.3119e-07, 1.4754e-06, 4.3864e-07, 1.8541e-06, 3.3732e-07, 1.3641e-06,\n",
      "         3.8881e-07, 1.1024e-06, 2.9837e-07, 3.2199e-07, 3.1042e-07],\n",
      "        [4.7179e-05, 1.4637e-06, 2.6546e-07, 6.1526e-07, 2.4709e-01, 1.1851e-06,\n",
      "         4.3524e-07, 4.1771e-01, 5.2591e-07, 3.1183e-07, 3.8616e-07, 1.5313e-03,\n",
      "         9.1308e-07, 3.6841e-07, 3.3361e-01, 2.2667e-07, 6.0247e-07, 4.6691e-07,\n",
      "         6.4712e-07, 1.3594e-06, 3.9160e-07, 1.6144e-06, 3.1748e-07, 1.2108e-06,\n",
      "         3.6012e-07, 9.9672e-07, 2.6401e-07, 2.8987e-07, 2.8537e-07]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.1294, grad_fn=<BinaryCrossEntropyBackward>) tensor([[9.9974e-01, 4.5052e-07, 6.3978e-07, 1.9447e-07, 3.4651e-05, 1.0427e-07,\n",
      "         2.0909e-07, 4.3970e-05, 8.9468e-07, 2.6320e-06, 1.6050e-06, 9.8361e-05,\n",
      "         3.6847e-07, 2.3917e-06, 6.1168e-05, 7.9146e-07, 1.5237e-07, 6.4746e-07,\n",
      "         4.4780e-07, 1.5593e-07, 7.1821e-07, 6.0367e-08, 3.6884e-07, 5.6862e-08,\n",
      "         1.7333e-06, 1.0169e-06, 3.3339e-06, 4.9189e-07, 1.5438e-06],\n",
      "        [7.3771e-05, 1.2919e-06, 1.9346e-07, 5.5427e-07, 1.1368e-01, 1.2871e-06,\n",
      "         3.2631e-07, 6.6981e-01, 4.1616e-07, 2.1154e-07, 2.2936e-07, 2.2065e-03,\n",
      "         9.6484e-07, 3.1537e-07, 2.1422e-01, 1.6983e-07, 5.8577e-07, 3.6418e-07,\n",
      "         5.8898e-07, 1.0802e-06, 3.8398e-07, 2.0146e-06, 2.3735e-07, 1.2038e-06,\n",
      "         2.3472e-07, 7.2681e-07, 2.4603e-07, 2.5146e-07, 1.9700e-07],\n",
      "        [7.4394e-05, 1.2962e-06, 1.9588e-07, 5.5451e-07, 1.0931e-01, 1.2951e-06,\n",
      "         3.2289e-07, 6.7859e-01, 4.0926e-07, 2.0816e-07, 2.2752e-07, 2.1311e-03,\n",
      "         9.5458e-07, 3.1513e-07, 2.0989e-01, 1.7148e-07, 5.7543e-07, 3.6867e-07,\n",
      "         5.7994e-07, 1.0618e-06, 3.9084e-07, 2.0012e-06, 2.3969e-07, 1.1884e-06,\n",
      "         2.3691e-07, 7.3439e-07, 2.4101e-07, 2.4414e-07, 2.0307e-07],\n",
      "        [6.7993e-05, 1.1664e-06, 1.8007e-07, 5.0200e-07, 1.0767e-01, 1.1414e-06,\n",
      "         3.0248e-07, 6.7699e-01, 3.8347e-07, 1.9096e-07, 2.1395e-07, 2.0139e-03,\n",
      "         8.5205e-07, 2.8108e-07, 2.1324e-01, 1.5520e-07, 5.0842e-07, 3.2541e-07,\n",
      "         5.3655e-07, 9.4769e-07, 3.5631e-07, 1.7907e-06, 2.2387e-07, 1.0666e-06,\n",
      "         2.0884e-07, 6.6459e-07, 2.1978e-07, 2.3154e-07, 1.8520e-07],\n",
      "        [6.7963e-05, 1.2031e-06, 1.8345e-07, 5.2244e-07, 1.0972e-01, 1.2078e-06,\n",
      "         3.0081e-07, 6.7465e-01, 3.8594e-07, 1.9446e-07, 2.1163e-07, 2.0481e-03,\n",
      "         8.7978e-07, 2.8970e-07, 2.1350e-01, 1.5830e-07, 5.2871e-07, 3.3624e-07,\n",
      "         5.4023e-07, 9.8519e-07, 3.6087e-07, 1.8607e-06, 2.1735e-07, 1.1141e-06,\n",
      "         2.1700e-07, 6.7037e-07, 2.2319e-07, 2.3552e-07, 1.8681e-07],\n",
      "        [7.0373e-05, 1.2210e-06, 1.9111e-07, 5.2857e-07, 1.0808e-01, 1.2127e-06,\n",
      "         3.1180e-07, 6.7743e-01, 4.0027e-07, 1.9670e-07, 2.1734e-07, 2.0791e-03,\n",
      "         9.0939e-07, 3.0073e-07, 2.1233e-01, 1.6325e-07, 5.4774e-07, 3.5102e-07,\n",
      "         5.5769e-07, 1.0108e-06, 3.7216e-07, 1.9185e-06, 2.2730e-07, 1.1214e-06,\n",
      "         2.2518e-07, 6.9422e-07, 2.3197e-07, 2.3717e-07, 1.9434e-07]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.0968, grad_fn=<BinaryCrossEntropyBackward>) tensor([[9.9992e-01, 8.7543e-08, 1.5457e-07, 3.6307e-08, 1.0176e-05, 1.8406e-08,\n",
      "         4.1458e-08, 1.3758e-05, 1.9399e-07, 7.3793e-07, 3.9583e-07, 3.3707e-05,\n",
      "         7.0784e-08, 6.5270e-07, 2.2251e-05, 1.9124e-07, 2.8450e-08, 1.3954e-07,\n",
      "         8.9747e-08, 2.9098e-08, 1.4452e-07, 1.0076e-08, 7.5796e-08, 9.8726e-09,\n",
      "         4.0961e-07, 2.1527e-07, 8.6489e-07, 1.0668e-07, 3.9523e-07],\n",
      "        [2.6332e-04, 3.2620e-06, 4.4075e-07, 1.5445e-06, 1.5697e-01, 3.8733e-06,\n",
      "         7.7411e-07, 4.6679e-01, 1.0562e-06, 4.1522e-07, 4.2124e-07, 1.7089e-02,\n",
      "         3.0352e-06, 7.8740e-07, 3.5885e-01, 4.0224e-07, 1.7799e-06, 8.6141e-07,\n",
      "         1.6594e-06, 2.4840e-06, 1.0747e-06, 6.5529e-06, 5.8622e-07, 3.7175e-06,\n",
      "         4.7106e-07, 1.5096e-06, 6.6532e-07, 6.9274e-07, 4.4640e-07],\n",
      "        [2.6462e-04, 3.3048e-06, 4.4438e-07, 1.5957e-06, 1.5863e-01, 3.9199e-06,\n",
      "         7.7499e-07, 4.6528e-01, 1.0810e-06, 4.2246e-07, 4.3101e-07, 1.7090e-02,\n",
      "         3.0894e-06, 7.9544e-07, 3.5869e-01, 4.0520e-07, 1.7871e-06, 8.6699e-07,\n",
      "         1.6775e-06, 2.4965e-06, 1.0781e-06, 6.5610e-06, 5.8305e-07, 3.7480e-06,\n",
      "         4.6225e-07, 1.5098e-06, 6.6976e-07, 7.1428e-07, 4.5105e-07],\n",
      "        [2.6600e-04, 3.2382e-06, 4.2817e-07, 1.4955e-06, 1.6007e-01, 3.7050e-06,\n",
      "         7.6385e-07, 4.6812e-01, 1.0301e-06, 4.1148e-07, 4.1179e-07, 1.7187e-02,\n",
      "         2.9885e-06, 7.7432e-07, 3.5432e-01, 3.8944e-07, 1.7343e-06, 8.3965e-07,\n",
      "         1.6142e-06, 2.4220e-06, 1.0581e-06, 6.4353e-06, 5.7040e-07, 3.6421e-06,\n",
      "         4.5537e-07, 1.4520e-06, 6.5328e-07, 6.7494e-07, 4.3774e-07],\n",
      "        [2.5675e-04, 3.2114e-06, 4.2863e-07, 1.5240e-06, 1.5714e-01, 3.8058e-06,\n",
      "         7.6260e-07, 4.7327e-01, 1.0378e-06, 4.0309e-07, 4.1978e-07, 1.6723e-02,\n",
      "         2.9547e-06, 7.8151e-07, 3.5257e-01, 4.0125e-07, 1.7811e-06, 8.2945e-07,\n",
      "         1.6727e-06, 2.4416e-06, 1.0442e-06, 6.5552e-06, 5.7600e-07, 3.6979e-06,\n",
      "         4.6089e-07, 1.4575e-06, 6.4296e-07, 6.8651e-07, 4.4186e-07],\n",
      "        [2.8073e-04, 3.5009e-06, 4.6960e-07, 1.6604e-06, 1.6386e-01, 4.0913e-06,\n",
      "         8.3239e-07, 4.6727e-01, 1.1502e-06, 4.4696e-07, 4.4673e-07, 1.7883e-02,\n",
      "         3.2522e-06, 8.5957e-07, 3.5067e-01, 4.3644e-07, 1.9272e-06, 9.3255e-07,\n",
      "         1.7735e-06, 2.6250e-06, 1.1571e-06, 7.0645e-06, 6.1647e-07, 3.9539e-06,\n",
      "         5.0202e-07, 1.5865e-06, 7.1099e-07, 7.5246e-07, 4.8200e-07]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.0768, grad_fn=<BinaryCrossEntropyBackward>) tensor([[9.9997e-01, 1.7460e-08, 3.8265e-08, 6.9524e-09, 3.1432e-06, 3.3336e-09,\n",
      "         8.4328e-09, 4.3822e-06, 4.3174e-08, 2.1192e-07, 1.0007e-07, 1.1936e-05,\n",
      "         1.3949e-08, 1.8254e-07, 8.2434e-06, 4.7366e-08, 5.4525e-09, 3.0869e-08,\n",
      "         1.8461e-08, 5.5723e-09, 2.9845e-08, 1.7252e-09, 1.5980e-08, 1.7582e-09,\n",
      "         9.9327e-08, 4.6764e-08, 2.3002e-07, 2.3732e-08, 1.0366e-07],\n",
      "        [4.6745e-04, 4.3247e-06, 5.3178e-07, 2.2847e-06, 3.1103e-01, 5.8843e-06,\n",
      "         9.9460e-07, 1.0038e-01, 1.3979e-06, 4.5113e-07, 4.6630e-07, 1.1509e-01,\n",
      "         4.6386e-06, 9.4843e-07, 4.7298e-01, 4.9978e-07, 2.7493e-06, 1.0479e-06,\n",
      "         2.3331e-06, 3.1366e-06, 1.4751e-06, 1.0504e-05, 7.3522e-07, 5.8906e-06,\n",
      "         5.0907e-07, 1.7555e-06, 8.2459e-07, 9.6291e-07, 5.2215e-07],\n",
      "        [4.7958e-04, 4.6836e-06, 5.6688e-07, 2.5056e-06, 3.0733e-01, 6.4632e-06,\n",
      "         1.0629e-06, 1.0055e-01, 1.4924e-06, 4.8698e-07, 5.0377e-07, 1.1628e-01,\n",
      "         5.0657e-06, 1.0173e-06, 4.7530e-01, 5.4398e-07, 2.9539e-06, 1.1372e-06,\n",
      "         2.5269e-06, 3.4318e-06, 1.5649e-06, 1.1292e-05, 7.9477e-07, 6.2448e-06,\n",
      "         5.4740e-07, 1.9314e-06, 8.9237e-07, 1.0349e-06, 5.5751e-07],\n",
      "        [4.8169e-04, 4.6492e-06, 5.6634e-07, 2.4755e-06, 3.1065e-01, 6.2295e-06,\n",
      "         1.0516e-06, 9.6675e-02, 1.4652e-06, 4.8866e-07, 4.9485e-07, 1.1748e-01,\n",
      "         4.8934e-06, 1.0066e-06, 4.7466e-01, 5.3643e-07, 2.9231e-06, 1.1247e-06,\n",
      "         2.4933e-06, 3.3506e-06, 1.5555e-06, 1.1205e-05, 7.8812e-07, 6.2546e-06,\n",
      "         5.4941e-07, 1.8831e-06, 8.9531e-07, 1.0284e-06, 5.5660e-07],\n",
      "        [4.9144e-04, 4.6707e-06, 5.7230e-07, 2.4677e-06, 3.0971e-01, 6.3444e-06,\n",
      "         1.0670e-06, 9.7348e-02, 1.4809e-06, 4.9421e-07, 5.0116e-07, 1.1800e-01,\n",
      "         4.9815e-06, 1.0167e-06, 4.7438e-01, 5.4544e-07, 2.9462e-06, 1.1218e-06,\n",
      "         2.5126e-06, 3.3629e-06, 1.5896e-06, 1.1195e-05, 7.9411e-07, 6.3000e-06,\n",
      "         5.4541e-07, 1.9008e-06, 9.0380e-07, 1.0519e-06, 5.5534e-07],\n",
      "        [4.8479e-04, 4.7320e-06, 5.9132e-07, 2.5556e-06, 3.1308e-01, 6.4877e-06,\n",
      "         1.1096e-06, 9.7795e-02, 1.5489e-06, 4.9965e-07, 5.0407e-07, 1.1949e-01,\n",
      "         5.1498e-06, 1.0363e-06, 4.6909e-01, 5.5690e-07, 3.0734e-06, 1.1782e-06,\n",
      "         2.5969e-06, 3.5121e-06, 1.6090e-06, 1.1592e-05, 8.2686e-07, 6.4818e-06,\n",
      "         5.5700e-07, 1.9303e-06, 9.2157e-07, 1.0729e-06, 5.7138e-07]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0750, grad_fn=<BinaryCrossEntropyBackward>) tensor([[9.9999e-01, 3.5580e-09, 9.6656e-09, 1.3593e-09, 9.9490e-07, 6.1663e-10,\n",
      "         1.7517e-09, 1.4577e-06, 9.8180e-09, 6.2084e-08, 2.5821e-08, 4.3765e-06,\n",
      "         2.8074e-09, 5.2097e-08, 3.0605e-06, 1.1973e-08, 1.0677e-09, 6.9779e-09,\n",
      "         3.8795e-09, 1.0901e-09, 6.2959e-09, 3.0164e-10, 3.4404e-09, 3.1973e-10,\n",
      "         2.4605e-08, 1.0377e-08, 6.2444e-08, 5.3908e-09, 2.7734e-08],\n",
      "        [3.1775e-04, 2.0629e-06, 2.3250e-07, 1.1865e-06, 3.7402e-01, 3.1866e-06,\n",
      "         4.5734e-07, 1.4555e-02, 5.9530e-07, 1.7531e-07, 1.9743e-07, 4.4927e-01,\n",
      "         2.3000e-06, 3.6740e-07, 1.6181e-01, 2.1091e-07, 1.3789e-06, 4.4719e-07,\n",
      "         1.1066e-06, 1.4916e-06, 6.8182e-07, 5.9365e-06, 3.3213e-07, 3.1232e-06,\n",
      "         2.0268e-07, 7.9015e-07, 3.4479e-07, 4.3064e-07, 2.0461e-07],\n",
      "        [3.0811e-04, 1.9875e-06, 2.1853e-07, 1.1292e-06, 3.7982e-01, 2.9498e-06,\n",
      "         4.3912e-07, 1.4434e-02, 5.6743e-07, 1.6715e-07, 1.8878e-07, 4.4440e-01,\n",
      "         2.1647e-06, 3.4814e-07, 1.6101e-01, 2.0107e-07, 1.2891e-06, 4.2176e-07,\n",
      "         1.0565e-06, 1.4027e-06, 6.5078e-07, 5.5828e-06, 3.1735e-07, 2.9784e-06,\n",
      "         1.9270e-07, 7.4227e-07, 3.2650e-07, 4.0932e-07, 1.9728e-07],\n",
      "        [3.1496e-04, 2.0888e-06, 2.2990e-07, 1.1722e-06, 3.7635e-01, 3.1110e-06,\n",
      "         4.6547e-07, 1.4872e-02, 5.9630e-07, 1.7603e-07, 2.0037e-07, 4.4668e-01,\n",
      "         2.2928e-06, 3.6666e-07, 1.6175e-01, 2.0945e-07, 1.3743e-06, 4.4810e-07,\n",
      "         1.1073e-06, 1.5230e-06, 6.8148e-07, 6.0406e-06, 3.3072e-07, 3.1582e-06,\n",
      "         1.9692e-07, 7.6367e-07, 3.4317e-07, 4.3557e-07, 2.0549e-07],\n",
      "        [3.1040e-04, 2.0241e-06, 2.2536e-07, 1.1556e-06, 3.7264e-01, 2.9929e-06,\n",
      "         4.4486e-07, 1.4588e-02, 5.7711e-07, 1.7547e-07, 1.9143e-07, 4.5238e-01,\n",
      "         2.2566e-06, 3.5571e-07, 1.6005e-01, 2.0498e-07, 1.3462e-06, 4.3403e-07,\n",
      "         1.0718e-06, 1.4367e-06, 6.6594e-07, 5.8068e-06, 3.2167e-07, 3.0868e-06,\n",
      "         1.9549e-07, 7.4485e-07, 3.3405e-07, 4.2104e-07, 2.0380e-07],\n",
      "        [3.1507e-04, 2.0970e-06, 2.3034e-07, 1.2133e-06, 3.7668e-01, 3.2337e-06,\n",
      "         4.5722e-07, 1.4958e-02, 5.9993e-07, 1.7920e-07, 2.0141e-07, 4.4765e-01,\n",
      "         2.3148e-06, 3.7217e-07, 1.6038e-01, 2.1330e-07, 1.3663e-06, 4.5540e-07,\n",
      "         1.1184e-06, 1.5433e-06, 6.8944e-07, 6.0298e-06, 3.2837e-07, 3.1537e-06,\n",
      "         2.0943e-07, 7.9568e-07, 3.4757e-07, 4.3129e-07, 2.0653e-07]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.1061, grad_fn=<BinaryCrossEntropyBackward>) tensor([[1.0000e+00, 7.3808e-10, 2.4830e-09, 2.7042e-10, 3.0817e-07, 1.1608e-10,\n",
      "         3.7030e-10, 4.9696e-07, 2.2731e-09, 1.8494e-08, 6.7774e-09, 1.6460e-06,\n",
      "         5.7494e-10, 1.5124e-08, 1.1562e-06, 3.0789e-09, 2.1284e-10, 1.6059e-09,\n",
      "         8.2989e-10, 2.1705e-10, 1.3518e-09, 5.3664e-11, 7.5380e-10, 5.9163e-11,\n",
      "         6.2040e-09, 2.3440e-09, 1.7246e-08, 1.2461e-09, 7.5452e-09],\n",
      "        [9.7048e-05, 3.9677e-07, 4.0100e-08, 2.4278e-07, 1.3617e-01, 6.7329e-07,\n",
      "         8.5511e-08, 1.9144e-03, 9.9203e-08, 2.7093e-08, 3.4557e-08, 8.3883e-01,\n",
      "         4.4354e-07, 5.2596e-08, 2.2986e-02, 3.5103e-08, 2.7053e-07, 7.5650e-08,\n",
      "         2.0960e-07, 2.9577e-07, 1.2810e-07, 1.3721e-06, 6.0991e-08, 6.6595e-07,\n",
      "         3.2566e-08, 1.4386e-07, 5.5084e-08, 7.3761e-08, 3.2333e-08],\n",
      "        [9.6413e-05, 4.0966e-07, 4.0738e-08, 2.4941e-07, 1.3928e-01, 6.9056e-07,\n",
      "         8.7490e-08, 2.0106e-03, 1.0134e-07, 2.7974e-08, 3.5270e-08, 8.3596e-01,\n",
      "         4.5770e-07, 5.4753e-08, 2.2644e-02, 3.6037e-08, 2.7079e-07, 7.6450e-08,\n",
      "         2.1260e-07, 3.0095e-07, 1.2937e-07, 1.3981e-06, 6.1654e-08, 6.7076e-07,\n",
      "         3.3618e-08, 1.4505e-07, 5.5243e-08, 7.5990e-08, 3.2815e-08],\n",
      "        [9.2626e-05, 3.9037e-07, 3.9077e-08, 2.3841e-07, 1.4108e-01, 6.6839e-07,\n",
      "         8.4329e-08, 2.0534e-03, 9.8827e-08, 2.6376e-08, 3.3228e-08, 8.3305e-01,\n",
      "         4.3566e-07, 5.1504e-08, 2.3723e-02, 3.4628e-08, 2.6570e-07, 7.4410e-08,\n",
      "         2.0570e-07, 2.9493e-07, 1.2145e-07, 1.3618e-06, 5.9081e-08, 6.6553e-07,\n",
      "         3.2456e-08, 1.4072e-07, 5.3413e-08, 7.2088e-08, 3.1475e-08],\n",
      "        [9.5258e-05, 4.0498e-07, 4.0486e-08, 2.5199e-07, 1.3641e-01, 6.9857e-07,\n",
      "         8.7993e-08, 1.9897e-03, 1.0343e-07, 2.7630e-08, 3.4319e-08, 8.3891e-01,\n",
      "         4.5927e-07, 5.4294e-08, 2.2594e-02, 3.5271e-08, 2.7933e-07, 7.8713e-08,\n",
      "         2.1266e-07, 3.0269e-07, 1.2883e-07, 1.4237e-06, 6.1355e-08, 6.9676e-07,\n",
      "         3.3616e-08, 1.4468e-07, 5.5643e-08, 7.6163e-08, 3.3264e-08],\n",
      "        [9.8043e-05, 3.9811e-07, 4.0247e-08, 2.3931e-07, 1.4032e-01, 6.7482e-07,\n",
      "         8.3732e-08, 2.0497e-03, 9.8086e-08, 2.6817e-08, 3.4501e-08, 8.3388e-01,\n",
      "         4.3297e-07, 5.2985e-08, 2.3651e-02, 3.4626e-08, 2.6537e-07, 7.4775e-08,\n",
      "         2.0458e-07, 2.9137e-07, 1.2657e-07, 1.3603e-06, 5.9901e-08, 6.6289e-07,\n",
      "         3.2601e-08, 1.4114e-07, 5.4068e-08, 7.3475e-08, 3.2145e-08]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.1266, grad_fn=<BinaryCrossEntropyBackward>) tensor([[1.0000e+00, 1.5542e-10, 6.4700e-10, 5.4579e-11, 9.8717e-08, 2.2176e-11,\n",
      "         7.9431e-11, 1.7099e-07, 5.3427e-10, 5.5870e-09, 1.8047e-09, 6.2089e-07,\n",
      "         1.1948e-10, 4.4538e-09, 4.4395e-07, 8.0318e-10, 4.3068e-11, 3.7523e-10,\n",
      "         1.8020e-10, 4.3867e-11, 2.9461e-10, 9.6878e-12, 1.6760e-10, 1.1108e-11,\n",
      "         1.5878e-09, 5.3743e-10, 4.8330e-09, 2.9227e-10, 2.0816e-09],\n",
      "        [2.8903e-05, 7.6287e-08, 6.8865e-09, 4.8479e-08, 9.5819e-02, 1.4317e-07,\n",
      "         1.5496e-08, 6.4623e-04, 1.6466e-08, 4.0563e-09, 6.1325e-09, 8.9589e-01,\n",
      "         8.4421e-08, 7.3542e-09, 7.6105e-03, 5.7507e-09, 5.2923e-08, 1.2779e-08,\n",
      "         3.8178e-08, 6.1140e-08, 2.2474e-08, 3.1051e-07, 1.1041e-08, 1.4219e-07,\n",
      "         5.4377e-09, 2.5961e-08, 8.1686e-09, 1.2259e-08, 4.9715e-09],\n",
      "        [2.8891e-05, 7.8945e-08, 7.0015e-09, 5.0689e-08, 9.3017e-02, 1.5009e-07,\n",
      "         1.5843e-08, 5.9576e-04, 1.6648e-08, 4.2064e-09, 6.2404e-09, 8.9886e-01,\n",
      "         8.7548e-08, 7.6657e-09, 7.4987e-03, 5.9899e-09, 5.3798e-08, 1.3136e-08,\n",
      "         3.9266e-08, 6.2960e-08, 2.2845e-08, 3.2389e-07, 1.1457e-08, 1.4884e-07,\n",
      "         5.5811e-09, 2.6981e-08, 8.4478e-09, 1.2800e-08, 5.1215e-09],\n",
      "        [2.9658e-05, 8.0185e-08, 7.1170e-09, 5.1319e-08, 9.6318e-02, 1.5066e-07,\n",
      "         1.6175e-08, 6.1651e-04, 1.6660e-08, 4.1617e-09, 6.2819e-09, 8.9527e-01,\n",
      "         8.9024e-08, 7.7553e-09, 7.7662e-03, 6.0315e-09, 5.4294e-08, 1.3362e-08,\n",
      "         3.9214e-08, 6.4342e-08, 2.3225e-08, 3.2516e-07, 1.1764e-08, 1.4975e-07,\n",
      "         5.7722e-09, 2.7396e-08, 8.5731e-09, 1.2979e-08, 5.1520e-09],\n",
      "        [2.8746e-05, 7.9473e-08, 7.2613e-09, 5.1713e-08, 9.3705e-02, 1.5241e-07,\n",
      "         1.6637e-08, 5.9497e-04, 1.7620e-08, 4.1916e-09, 6.2270e-09, 8.9847e-01,\n",
      "         9.1676e-08, 7.7766e-09, 7.1994e-03, 6.0255e-09, 5.5025e-08, 1.3326e-08,\n",
      "         3.9535e-08, 6.4273e-08, 2.3324e-08, 3.2281e-07, 1.1676e-08, 1.5352e-07,\n",
      "         5.6413e-09, 2.6846e-08, 8.6034e-09, 1.3286e-08, 5.1994e-09],\n",
      "        [2.8990e-05, 7.8776e-08, 6.9999e-09, 5.0234e-08, 9.7055e-02, 1.4696e-07,\n",
      "         1.5915e-08, 6.5613e-04, 1.6408e-08, 4.1074e-09, 6.1130e-09, 8.9441e-01,\n",
      "         8.6619e-08, 7.4761e-09, 7.8447e-03, 5.8847e-09, 5.3704e-08, 1.3004e-08,\n",
      "         3.8705e-08, 6.3126e-08, 2.3028e-08, 3.1535e-07, 1.1662e-08, 1.4801e-07,\n",
      "         5.6187e-09, 2.6307e-08, 8.3061e-09, 1.2612e-08, 5.0417e-09]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.1006, grad_fn=<BinaryCrossEntropyBackward>) tensor([[1.0000e+00, 3.3150e-11, 1.7065e-10, 1.1152e-11, 3.5774e-08, 4.2897e-12,\n",
      "         1.7252e-11, 5.8858e-08, 1.2720e-10, 1.7083e-09, 4.8649e-10, 2.3105e-07,\n",
      "         2.5141e-11, 1.3278e-09, 1.7334e-07, 2.1212e-10, 8.8267e-12, 8.8810e-11,\n",
      "         3.9627e-11, 8.9781e-12, 6.5020e-11, 1.7707e-12, 3.7729e-11, 2.1114e-12,\n",
      "         4.1157e-10, 1.2479e-10, 1.3713e-09, 6.9412e-11, 5.8120e-10],\n",
      "        [1.0171e-05, 2.3706e-08, 1.9311e-09, 1.6596e-08, 6.0054e-01, 4.8671e-08,\n",
      "         4.7649e-09, 1.6056e-03, 4.1425e-09, 9.4540e-10, 2.0511e-09, 3.6540e-01,\n",
      "         2.3913e-08, 1.4502e-09, 3.2442e-02, 1.5332e-09, 1.6380e-08, 3.2866e-09,\n",
      "         1.1221e-08, 2.2045e-08, 5.7831e-09, 9.6995e-08, 3.6793e-09, 5.1406e-08,\n",
      "         1.6283e-09, 8.4360e-09, 1.7317e-09, 3.2651e-09, 1.2100e-09],\n",
      "        [9.8318e-06, 2.3204e-08, 1.8232e-09, 1.6199e-08, 5.8977e-01, 4.6763e-08,\n",
      "         4.5224e-09, 1.3072e-03, 3.9849e-09, 8.8553e-10, 1.9345e-09, 3.7882e-01,\n",
      "         2.3036e-08, 1.3822e-09, 3.0093e-02, 1.4657e-09, 1.5627e-08, 3.0352e-09,\n",
      "         1.0689e-08, 2.1242e-08, 5.4579e-09, 9.4695e-08, 3.5247e-09, 4.9995e-08,\n",
      "         1.5315e-09, 8.1081e-09, 1.6534e-09, 3.1624e-09, 1.1371e-09],\n",
      "        [1.0572e-05, 2.4568e-08, 2.0041e-09, 1.7055e-08, 5.8746e-01, 4.8727e-08,\n",
      "         4.9368e-09, 1.6126e-03, 4.2948e-09, 9.7641e-10, 2.1055e-09, 3.7757e-01,\n",
      "         2.4321e-08, 1.4892e-09, 3.3349e-02, 1.5952e-09, 1.6397e-08, 3.3491e-09,\n",
      "         1.1602e-08, 2.2825e-08, 6.0229e-09, 1.0217e-07, 3.8062e-09, 5.2891e-08,\n",
      "         1.6579e-09, 8.7425e-09, 1.7944e-09, 3.4389e-09, 1.2586e-09],\n",
      "        [9.7574e-06, 2.2958e-08, 1.7381e-09, 1.5778e-08, 5.9887e-01, 4.5789e-08,\n",
      "         4.4905e-09, 1.3772e-03, 3.9115e-09, 8.6709e-10, 1.8341e-09, 3.7234e-01,\n",
      "         2.3084e-08, 1.3475e-09, 2.7403e-02, 1.4130e-09, 1.5740e-08, 3.1140e-09,\n",
      "         1.0818e-08, 2.0981e-08, 5.4449e-09, 9.4548e-08, 3.5373e-09, 4.9887e-08,\n",
      "         1.5326e-09, 7.8731e-09, 1.6282e-09, 3.0246e-09, 1.1075e-09],\n",
      "        [1.0595e-05, 2.2883e-08, 1.8749e-09, 1.5936e-08, 6.1002e-01, 4.5944e-08,\n",
      "         4.5548e-09, 1.6336e-03, 3.9635e-09, 9.1153e-10, 1.9317e-09, 3.5553e-01,\n",
      "         2.2872e-08, 1.3777e-09, 3.2799e-02, 1.4978e-09, 1.5516e-08, 3.1289e-09,\n",
      "         1.0645e-08, 2.1573e-08, 5.5328e-09, 9.4693e-08, 3.7060e-09, 4.9462e-08,\n",
      "         1.5727e-09, 8.0693e-09, 1.6874e-09, 3.1586e-09, 1.1552e-09]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1211, grad_fn=<BinaryCrossEntropyBackward>) tensor([[1.0000e+00, 7.1486e-12, 4.5484e-11, 2.3029e-12, 1.3382e-08, 8.3874e-13,\n",
      "         3.7873e-12, 2.0263e-08, 3.0621e-11, 5.2777e-10, 1.3254e-10, 8.5689e-08,\n",
      "         5.3472e-12, 4.0009e-10, 6.8996e-08, 5.6618e-11, 1.8290e-12, 2.1255e-11,\n",
      "         8.8100e-12, 1.8576e-12, 1.4507e-11, 3.2710e-13, 8.5851e-12, 4.0561e-13,\n",
      "         1.0785e-10, 2.9297e-11, 3.9328e-10, 1.6663e-11, 1.6395e-10],\n",
      "        [1.8345e-06, 9.4782e-09, 1.0303e-09, 7.6394e-09, 4.5517e-01, 1.6830e-08,\n",
      "         2.3125e-09, 1.2143e-02, 1.5097e-09, 3.9343e-10, 1.3789e-09, 9.6968e-03,\n",
      "         7.8102e-09, 4.3377e-10, 5.2298e-01, 7.6386e-10, 6.4118e-09, 1.3384e-09,\n",
      "         4.6559e-09, 1.2402e-08, 2.2694e-09, 2.7477e-08, 2.5251e-09, 2.0377e-08,\n",
      "         1.0037e-09, 4.5909e-09, 6.1158e-10, 1.4399e-09, 5.6008e-10],\n",
      "        [1.7977e-06, 1.0724e-08, 1.1755e-09, 8.7620e-09, 4.2436e-01, 1.9393e-08,\n",
      "         2.5680e-09, 1.4531e-02, 1.7137e-09, 4.6568e-10, 1.6574e-09, 7.4048e-03,\n",
      "         8.6928e-09, 4.8329e-10, 5.5370e-01, 8.8049e-10, 7.0313e-09, 1.5308e-09,\n",
      "         5.2135e-09, 1.3806e-08, 2.5845e-09, 2.9658e-08, 2.8712e-09, 2.2122e-08,\n",
      "         1.2251e-09, 5.3128e-09, 6.6148e-10, 1.6338e-09, 6.4319e-10],\n",
      "        [1.8739e-06, 1.1207e-08, 1.2274e-09, 9.0890e-09, 4.2450e-01, 1.9667e-08,\n",
      "         2.7635e-09, 1.4861e-02, 1.8252e-09, 4.8926e-10, 1.6819e-09, 7.6164e-03,\n",
      "         9.0742e-09, 5.1530e-10, 5.5302e-01, 9.0355e-10, 7.4034e-09, 1.6440e-09,\n",
      "         5.3574e-09, 1.4480e-08, 2.6570e-09, 3.0780e-08, 2.9538e-09, 2.3229e-08,\n",
      "         1.2726e-09, 5.4205e-09, 6.8776e-10, 1.7209e-09, 6.8180e-10],\n",
      "        [1.9949e-06, 1.0463e-08, 1.1381e-09, 8.3130e-09, 4.3649e-01, 1.8531e-08,\n",
      "         2.4400e-09, 1.2601e-02, 1.6476e-09, 4.4660e-10, 1.5599e-09, 8.5174e-03,\n",
      "         8.2689e-09, 4.8305e-10, 5.4239e-01, 8.6465e-10, 6.7893e-09, 1.4888e-09,\n",
      "         4.9911e-09, 1.3096e-08, 2.4829e-09, 2.9311e-08, 2.7774e-09, 2.1561e-08,\n",
      "         1.1913e-09, 5.1244e-09, 6.8406e-10, 1.6099e-09, 6.2717e-10],\n",
      "        [2.4020e-06, 1.2710e-08, 1.3856e-09, 9.8967e-09, 4.3972e-01, 2.1929e-08,\n",
      "         2.9013e-09, 1.7305e-02, 2.0275e-09, 5.5748e-10, 1.9076e-09, 9.5450e-03,\n",
      "         9.9510e-09, 5.6809e-10, 5.3343e-01, 1.0192e-09, 7.8616e-09, 1.7867e-09,\n",
      "         5.9677e-09, 1.6034e-08, 3.0409e-09, 3.6053e-08, 3.1917e-09, 2.4849e-08,\n",
      "         1.3587e-09, 6.1408e-09, 8.2828e-10, 1.8585e-09, 7.2608e-10]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.1699, grad_fn=<BinaryCrossEntropyBackward>) tensor([[1.0000e+00, 1.5564e-12, 1.2235e-11, 4.7993e-13, 4.7105e-09, 1.6554e-13,\n",
      "         8.3923e-13, 7.0389e-09, 7.4430e-12, 1.6454e-10, 3.6447e-11, 3.1852e-08,\n",
      "         1.1479e-12, 1.2168e-10, 2.7737e-08, 1.5254e-11, 3.8262e-13, 5.1365e-12,\n",
      "         1.9774e-12, 3.8802e-13, 3.2674e-12, 6.0991e-14, 1.9718e-12, 7.8642e-14,\n",
      "         2.8535e-11, 6.9442e-12, 1.1386e-10, 4.0376e-12, 4.6669e-11],\n",
      "        [1.1884e-06, 1.5354e-08, 2.7619e-09, 1.3946e-08, 2.1751e-02, 2.3886e-08,\n",
      "         4.5636e-09, 8.5636e-02, 2.7268e-09, 9.8815e-10, 3.7225e-09, 7.8980e-04,\n",
      "         1.2419e-08, 7.5082e-10, 8.9182e-01, 1.8840e-09, 1.0275e-08, 3.1873e-09,\n",
      "         8.7843e-09, 2.5525e-08, 4.7575e-09, 3.4746e-08, 6.5092e-09, 2.6451e-08,\n",
      "         2.8717e-09, 9.7255e-09, 1.2794e-09, 3.1077e-09, 1.4089e-09],\n",
      "        [1.1724e-06, 1.5589e-08, 2.7469e-09, 1.3786e-08, 2.1476e-02, 2.4167e-08,\n",
      "         4.5198e-09, 7.8082e-02, 2.7081e-09, 9.8987e-10, 3.7561e-09, 7.6715e-04,\n",
      "         1.2289e-08, 7.5975e-10, 8.9967e-01, 1.8412e-09, 1.0056e-08, 3.0562e-09,\n",
      "         8.4662e-09, 2.5757e-08, 4.6998e-09, 3.4840e-08, 6.2367e-09, 2.6127e-08,\n",
      "         2.7833e-09, 9.7820e-09, 1.2535e-09, 3.1098e-09, 1.3681e-09],\n",
      "        [1.6950e-06, 1.9898e-08, 3.3981e-09, 1.6426e-08, 2.8218e-02, 3.1056e-08,\n",
      "         5.2876e-09, 1.1339e-01, 3.3513e-09, 1.2007e-09, 4.7545e-09, 1.1668e-03,\n",
      "         1.5063e-08, 9.2294e-10, 8.5722e-01, 2.2261e-09, 1.1460e-08, 3.6736e-09,\n",
      "         1.0653e-08, 2.9901e-08, 6.0317e-09, 4.6493e-08, 7.3832e-09, 3.2322e-08,\n",
      "         3.3153e-09, 1.2115e-08, 1.6120e-09, 3.7715e-09, 1.6682e-09],\n",
      "        [1.2380e-06, 1.5386e-08, 2.5935e-09, 1.3047e-08, 2.1720e-02, 2.3161e-08,\n",
      "         4.2079e-09, 7.4957e-02, 2.4913e-09, 9.0054e-10, 3.4264e-09, 8.1854e-04,\n",
      "         1.1633e-08, 7.1272e-10, 9.0250e-01, 1.7074e-09, 9.3686e-09, 2.8988e-09,\n",
      "         7.9117e-09, 2.3509e-08, 4.5150e-09, 3.4527e-08, 5.9235e-09, 2.5297e-08,\n",
      "         2.5237e-09, 9.4519e-09, 1.1758e-09, 2.9834e-09, 1.2787e-09],\n",
      "        [1.3389e-06, 2.0515e-08, 3.6606e-09, 1.7753e-08, 1.9316e-02, 3.0285e-08,\n",
      "         5.7150e-09, 9.7894e-02, 3.3629e-09, 1.2954e-09, 4.9720e-09, 6.5044e-04,\n",
      "         1.5848e-08, 9.9961e-10, 8.8214e-01, 2.4605e-09, 1.2431e-08, 4.0012e-09,\n",
      "         1.0826e-08, 3.2555e-08, 5.9613e-09, 4.4033e-08, 8.0854e-09, 3.4009e-08,\n",
      "         3.6182e-09, 1.2582e-08, 1.6305e-09, 4.2591e-09, 1.8555e-09]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.0848, grad_fn=<BinaryCrossEntropyBackward>) tensor([[1.0000e+00, 3.4175e-13, 3.3181e-12, 1.0083e-13, 1.6595e-09, 3.2941e-14,\n",
      "         1.8749e-13, 2.5196e-09, 1.8247e-12, 5.1713e-11, 1.0105e-11, 1.1907e-08,\n",
      "         2.4846e-13, 3.7312e-11, 1.0987e-08, 4.1433e-12, 8.0718e-14, 1.2520e-12,\n",
      "         4.4755e-13, 8.1726e-14, 7.4205e-13, 1.1466e-14, 4.5661e-13, 1.5372e-14,\n",
      "         7.6133e-12, 1.6598e-12, 3.3239e-11, 9.8645e-13, 1.3390e-11],\n",
      "        [5.8111e-06, 1.4319e-08, 1.2182e-09, 1.0384e-08, 5.7006e-01, 3.0927e-08,\n",
      "         2.5985e-09, 2.2510e-01, 1.6400e-09, 3.6568e-10, 1.3101e-09, 5.7018e-02,\n",
      "         1.4410e-08, 3.7666e-10, 1.4782e-01, 7.6326e-10, 8.4699e-09, 1.7302e-09,\n",
      "         6.3529e-09, 1.7246e-08, 3.7958e-09, 7.3790e-08, 2.6674e-09, 3.0559e-08,\n",
      "         9.3742e-10, 5.0035e-09, 6.9469e-10, 1.6474e-09, 5.2292e-10],\n",
      "        [6.4719e-06, 1.6260e-08, 1.4254e-09, 1.1490e-08, 5.1433e-01, 3.3778e-08,\n",
      "         2.9260e-09, 2.8253e-01, 1.8471e-09, 4.2526e-10, 1.5509e-09, 5.1822e-02,\n",
      "         1.6128e-08, 4.3030e-10, 1.5131e-01, 8.7415e-10, 9.3019e-09, 2.0302e-09,\n",
      "         7.1204e-09, 1.9156e-08, 4.3269e-09, 7.9975e-08, 3.0857e-09, 3.2848e-08,\n",
      "         1.1028e-09, 5.6876e-09, 8.0991e-10, 1.8579e-09, 6.0902e-10],\n",
      "        [6.9437e-06, 1.4307e-08, 1.2016e-09, 1.0011e-08, 5.9141e-01, 3.0332e-08,\n",
      "         2.4343e-09, 1.9380e-01, 1.5887e-09, 3.5469e-10, 1.2752e-09, 6.5778e-02,\n",
      "         1.4035e-08, 3.8290e-10, 1.4900e-01, 7.3619e-10, 7.8914e-09, 1.6790e-09,\n",
      "         6.0985e-09, 1.6129e-08, 3.8044e-09, 7.3196e-08, 2.5426e-09, 2.9909e-08,\n",
      "         8.7323e-10, 4.8731e-09, 7.2407e-10, 1.6599e-09, 5.0866e-10],\n",
      "        [6.0372e-06, 1.3829e-08, 1.1993e-09, 9.9791e-09, 5.5552e-01, 2.8743e-08,\n",
      "         2.5061e-09, 2.2519e-01, 1.5973e-09, 3.5886e-10, 1.3339e-09, 6.1114e-02,\n",
      "         1.3863e-08, 3.7548e-10, 1.5817e-01, 7.2767e-10, 7.9982e-09, 1.7006e-09,\n",
      "         6.3271e-09, 1.6109e-08, 3.6994e-09, 6.8465e-08, 2.6269e-09, 2.8949e-08,\n",
      "         8.9179e-10, 4.7171e-09, 6.9264e-10, 1.6061e-09, 5.1773e-10],\n",
      "        [6.3249e-06, 1.6066e-08, 1.4284e-09, 1.1568e-08, 5.1919e-01, 3.4068e-08,\n",
      "         2.9088e-09, 2.6859e-01, 1.8941e-09, 4.3901e-10, 1.5983e-09, 5.6907e-02,\n",
      "         1.6103e-08, 4.4241e-10, 1.5531e-01, 8.8518e-10, 9.3655e-09, 2.0285e-09,\n",
      "         7.3771e-09, 1.8991e-08, 4.3722e-09, 7.8152e-08, 3.0590e-09, 3.3236e-08,\n",
      "         1.0762e-09, 5.4910e-09, 8.2301e-10, 1.9092e-09, 6.2024e-10]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.0000e+00, 8.7884e-14, 1.0316e-12, 2.4913e-14, 6.4372e-10, 7.7519e-15,\n",
      "         4.8963e-14, 1.0237e-09, 5.1819e-13, 1.8359e-11, 3.2048e-12, 4.9569e-09,\n",
      "         6.3062e-14, 1.2958e-11, 4.7859e-09, 1.2902e-12, 2.0023e-14, 3.5372e-13,\n",
      "         1.1827e-13, 2.0243e-14, 1.9670e-13, 2.5634e-15, 1.2316e-13, 3.5593e-15,\n",
      "         2.3331e-12, 4.6075e-13, 1.1045e-11, 2.7922e-13, 4.3789e-12],\n",
      "        [3.9857e-06, 3.2295e-09, 1.8878e-10, 2.2099e-09, 7.7329e-01, 8.4535e-09,\n",
      "         4.8283e-10, 3.3165e-02, 3.1572e-10, 5.0597e-11, 1.6429e-10, 1.8778e-01,\n",
      "         3.9446e-09, 6.9565e-11, 5.7609e-03, 1.1608e-10, 1.9339e-09, 3.0652e-10,\n",
      "         1.2745e-09, 3.0936e-09, 8.1925e-10, 2.7602e-08, 4.0778e-10, 8.8205e-09,\n",
      "         1.1312e-10, 7.2635e-10, 1.3237e-10, 3.1747e-10, 7.6795e-11],\n",
      "        [4.3342e-06, 3.3664e-09, 1.9938e-10, 2.2503e-09, 7.6917e-01, 8.8375e-09,\n",
      "         5.0282e-10, 3.5876e-02, 3.2663e-10, 5.3817e-11, 1.7415e-10, 1.8917e-01,\n",
      "         4.0064e-09, 7.3517e-11, 5.7823e-03, 1.2200e-10, 1.9749e-09, 3.1257e-10,\n",
      "         1.3007e-09, 3.2215e-09, 8.5719e-10, 2.9980e-08, 4.3132e-10, 9.0195e-09,\n",
      "         1.1578e-10, 7.8408e-10, 1.3857e-10, 3.2383e-10, 7.9679e-11],\n",
      "        [4.3571e-06, 3.6016e-09, 2.0538e-10, 2.3885e-09, 7.6587e-01, 9.6384e-09,\n",
      "         5.3211e-10, 3.6922e-02, 3.4666e-10, 5.6490e-11, 1.8485e-10, 1.9147e-01,\n",
      "         4.2804e-09, 7.7435e-11, 5.7321e-03, 1.2705e-10, 2.1222e-09, 3.4348e-10,\n",
      "         1.3945e-09, 3.4434e-09, 9.2665e-10, 3.1558e-08, 4.5147e-10, 9.5081e-09,\n",
      "         1.2363e-10, 8.2952e-10, 1.4780e-10, 3.3738e-10, 8.3132e-11],\n",
      "        [4.0199e-06, 3.2645e-09, 1.9082e-10, 2.2665e-09, 7.6836e-01, 8.6854e-09,\n",
      "         5.0434e-10, 3.2939e-02, 3.2611e-10, 5.2201e-11, 1.6436e-10, 1.9299e-01,\n",
      "         4.0148e-09, 7.1731e-11, 5.7099e-03, 1.1944e-10, 1.9953e-09, 3.1537e-10,\n",
      "         1.3204e-09, 3.2246e-09, 8.3844e-10, 2.8997e-08, 4.1336e-10, 9.0821e-09,\n",
      "         1.1360e-10, 7.5299e-10, 1.3770e-10, 3.1972e-10, 7.8990e-11],\n",
      "        [3.9921e-06, 3.2173e-09, 1.8214e-10, 2.1576e-09, 7.7288e-01, 8.3673e-09,\n",
      "         4.7559e-10, 3.1164e-02, 3.0660e-10, 4.9390e-11, 1.5629e-10, 1.9022e-01,\n",
      "         3.9173e-09, 6.7860e-11, 5.7385e-03, 1.1466e-10, 1.9462e-09, 3.0576e-10,\n",
      "         1.2785e-09, 3.0861e-09, 8.0488e-10, 2.8110e-08, 4.0993e-10, 8.5878e-09,\n",
      "         1.0816e-10, 7.2584e-10, 1.3172e-10, 3.0556e-10, 7.4243e-11]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "transformer = Transformer()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0002)\n",
    "loss = torch.nn.BCELoss()\n",
    "\n",
    "real_sample = torch.Tensor([\n",
    "        [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0]\n",
    "])\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    sample = transformer()        \n",
    "    # target = torch.ones(sample.shape[0], sample.shape[1])\n",
    "    error = loss(sample, real_sample)\n",
    "    if _ % 10 == 0: print(error, sample)\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    if(error > 8): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  7,  4, 11, 11, 14])\n",
      "tensor([0, 4, 4, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(real_sample, dim=1))\n",
    "print(torch.argmax(sample, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
