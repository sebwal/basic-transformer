{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DELETE THIS EVENTUALLY\n",
    "# def clones(module, N):\n",
    "#     \"Produce N identical layers.\"\n",
    "#     return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, masked=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.masked = masked\n",
    "        self.attentionHeads = nn.ModuleList([SingleHeadAttention(masked) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(ATTENTION_CONST['mh_concat_width'], ATTENTION_CONST['mh_output_width'])\n",
    "        self.lastHeadKV = None\n",
    "\n",
    "    def forward(self, inputs, encoderKV=None):\n",
    "        x = []\n",
    "        for head in self.attentionHeads:\n",
    "            sh_attention, k, v = head(inputs, encoderKV=encoderKV) \n",
    "            x.append(sh_attention)\n",
    "        self.lastHeadKV = {'K': k,'V': v}\n",
    "        x = torch.cat(x, 1) # concatinate all single head attention outputs\n",
    "        x = self.linear(x) # matmul with weight matrix (linear layer) to get 10x64 shape\n",
    "        return x\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, masked):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.masked = masked\n",
    "        self.linear1 = nn.Linear(ATTENTION_CONST['sh_linear1_input'], ATTENTION_CONST['sh_linear1_output'])\n",
    "        self.linear2 = nn.Linear(ATTENTION_CONST['sh_linear2_input'], ATTENTION_CONST['sh_linear2_output'])\n",
    "        self.linear3 = nn.Linear(ATTENTION_CONST['sh_linear3_input'], ATTENTION_CONST['sh_linear3_output'])\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([ATTENTION_CONST['sh_scale_factor']]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, encoderKV=None):        \n",
    "        q = self.linear1(inputs)\n",
    "        k = self.linear2(inputs) if encoderKV == None else encoderKV['K']\n",
    "        v = self.linear3(inputs) if encoderKV == None else encoderKV['V']\n",
    "        x = torch.matmul(q, k.permute(1, 0)) \n",
    "        x = x * self.scale\n",
    "        # if self.masked:\n",
    "        #     # TODO \"future positions\" have to be set to -inf. this is for the decoder to only allow self attention to consider earlier positions.\n",
    "        x = self.softmax(x) \n",
    "        x = torch.matmul(x, v)\n",
    "        return x if encoderKV != None else x, k, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(64, 512)\n",
    "        self.wQ = nn.Linear(512, 64)\n",
    "        self.wK = nn.Linear(512, 64)\n",
    "        self.wV = nn.Linear(512, 64)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        q = self.wQ(q)\n",
    "        k = self.wK(k)\n",
    "        v = self.wV(v)\n",
    "\n",
    "        # split heads - I think they do this instead of a loop\n",
    "        x, attention_weights = self.applyHeads(q, k, v)\n",
    "        # transpose ?\n",
    "        # reshape ?\n",
    "        x = self.linear(x)\n",
    "        return x, attention_weights\n",
    "\n",
    "    def applyHeads(self, q, k, v, mask=None):\n",
    "        x = torch.matmul(q, k.permute(1, 0)) \n",
    "        # scale x\n",
    "        # add mask\n",
    "        attention_weights = nn.Softmax(dim=-1)(x)\n",
    "        x = torch.matmul(attention_weights, v)\n",
    "        return x, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model=512, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = nn.ModuleList([copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)])\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoderLayers = nn.ModuleList([EncoderLayer(n_heads) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(512)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        x = inputs\n",
    "\n",
    "        for layer in self.encoderLayers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mhattention = MultiHeadAttention(n_heads)\n",
    "        self.norm1 = nn.LayerNorm(512)\n",
    "        self.norm2 = nn.LayerNorm(512)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.feedforward = PositionwiseFeedForward(512, 2048)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        x = inputs \n",
    "        z = x\n",
    "        x = self.mhattention(x, x, x, mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = z + x\n",
    "        x = self.norm1(x)\n",
    "        z = x\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = z + x\n",
    "        x = self.norm2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoderLayers = nn.ModuleList([DecoderLayer(n_heads) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(512)\n",
    "\n",
    "    def forward(self, inputs, encoderOut, src_mask, tgt_mask):\n",
    "        x = inputs\n",
    "\n",
    "        for layer in self.decoderLayers:\n",
    "            x = layer(x, encoderOut, src_mask, tgt_mask) \n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mhattention1 = MultiHeadAttention(n_heads)\n",
    "        self.mhattention2 = MultiHeadAttention(n_heads)\n",
    "        self.norm1 = nn.LayerNorm(512)\n",
    "        self.norm2 = nn.LayerNorm(512)\n",
    "        self.norm3 = nn.LayerNorm(512)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.feedforward = PositionwiseFeedForward(512, 2048)\n",
    "\n",
    "    def forward(self, inputs, encoderOut, src_mask, tgt_mask):\n",
    "        x = inputs\n",
    "        z = x\n",
    "        x = self.mhattention1(x, x, x, tgt_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = z + x        \n",
    "        x = self.norm1(x)\n",
    "        z = x\n",
    "        x = self.mhattention2(x, encoderOut, encoderOut, src_mask) \n",
    "        x = self.dropout2(x)\n",
    "        x = z + x\n",
    "        x = self.norm2(x)\n",
    "        z = x\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = z + x\n",
    "        x = self.norm3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, dim_model, dim_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim_model, dim_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_ff, dim_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.linear1(x)\n",
    "        x = nn.functional.relu(x) \n",
    "        x = self.dropout(x) \n",
    "        x = self.linear2(x) \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0.0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (Outer layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, n_layers=6, n_heads=8):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(n_layers, n_heads)\n",
    "        self.decoder = Decoder(n_layers, n_heads)\n",
    "        self.src_embed = nn.Sequential(Embeddings(512, src_vocab), PositionalEncoding(512, 0.1))\n",
    "        self.tgt_embed = nn.Sequential(Embeddings(512, tgt_vocab), PositionalEncoding(512, 0.1))\n",
    "        self.proj = nn.Linear(512, tgt_vocab)\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform(p)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        x = self.encoder(self.src_embed(src), src_mask)\n",
    "        x = self.decoder(self.tgt_embed(tgt), x, src_mask, tgt_mask)\n",
    "        x = self.finalize_output(x)\n",
    "        return x\n",
    "    \n",
    "    def greedy_decode(self, src, src_mask, max_len, start_symbol):\n",
    "        encoderOut = self.encoder(self.src_embed(src), src_mask)\n",
    "        ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "        for i in range(max_len-1):\n",
    "            tgt = Variable(ys)\n",
    "            tgt_mask = Variable(subsequent_mask(ys.size(1)).type_as(src.data))\n",
    "            out = self.decoder(self.tgt_embed(tgt), encoderOut, src_mask, tgt_mask) \n",
    "            prob = self.finalize_output(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim = 1)\n",
    "            next_word = next_word.data[0]\n",
    "            ys = torch.cat([ys, \n",
    "                            torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        return ys    \n",
    "    \n",
    "    def finalize_output(self, inputs): \n",
    "        x = F.log_softmax(self.proj(inputs), dim=-1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(11, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch + Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 1: # Changed > 0 to > 1 because it throws errors otherwise            \n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Paste Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, criterion, opt=None):\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        norm = norm.float() # because it throws errors if it's not casted to float\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        ret = loss.data[0] * norm\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10))).long()\n",
    "        print('1', data)\n",
    "        data[:, 0] = 1\n",
    "        print('2', data)\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "tensor(1.3218)\n",
      "epoch 2\n",
      "tensor(0.0183)\n",
      "epoch 3\n",
      "tensor(0.0006)\n",
      "epoch 4\n",
      "tensor(0.0001)\n",
      "epoch 5\n",
      "tensor(0.0001)\n",
      "epoch 6\n",
      "tensor(0.0000)\n",
      "epoch 7\n",
      "tensor(0.0000)\n",
      "epoch 8\n",
      "tensor(8.9222e-06)\n",
      "epoch 9\n",
      "tensor(5.5313e-06)\n",
      "epoch 10\n",
      "tensor(3.4544e-06)\n",
      "epoch 11\n",
      "tensor(2.5855e-06)\n",
      "epoch 12\n",
      "tensor(1.7696e-06)\n",
      "epoch 13\n",
      "tensor(1.9974e-06)\n",
      "epoch 14\n",
      "tensor(1.3669e-06)\n",
      "epoch 15\n",
      "tensor(7.5234e-07)\n",
      "epoch 16\n",
      "tensor(5.8280e-07)\n",
      "epoch 17\n",
      "tensor(7.4704e-07)\n",
      "epoch 18\n",
      "tensor(4.4505e-07)\n",
      "epoch 19\n",
      "tensor(4.3975e-07)\n",
      "epoch 20\n",
      "tensor(4.3445e-07)\n",
      "epoch 21\n",
      "tensor(3.6028e-07)\n",
      "epoch 22\n",
      "tensor(2.2252e-07)\n",
      "epoch 23\n",
      "tensor(1.4835e-07)\n",
      "epoch 24\n",
      "tensor(1.5895e-07)\n",
      "epoch 25\n",
      "tensor(1.9073e-07)\n",
      "epoch 26\n",
      "tensor(2.1193e-07)\n",
      "epoch 27\n",
      "tensor(2.1723e-07)\n",
      "epoch 28\n",
      "tensor(7.9473e-08)\n",
      "epoch 29\n",
      "tensor(3.0200e-07)\n",
      "epoch 30\n",
      "tensor(4.5035e-07)\n"
     ]
    }
   ],
   "source": [
    "# Train the simple copy task.\n",
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "transformer = Transformer(V, V, 2)\n",
    "model_opt = NoamOpt(transformer.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "losses = []\n",
    "for epoch in range(30):\n",
    "    transformer.train()\n",
    "\n",
    "    data_iter = data_gen(V, 30, 20)\n",
    "    loss_compute = SimpleLossCompute(criterion, model_opt)\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = transformer.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "    loss = total_loss / total_tokens.float()\n",
    "    losses.append(loss)\n",
    "    transformer.eval()\n",
    "\n",
    "    print('epoch ' + str(epoch + 1))\n",
    "    print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 28\n"
     ]
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "print('Best epoch: ' + str(losses.index(min(losses)) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "src1 = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]))\n",
    "src2 = Variable(torch.LongTensor([[1,1,1,3,3,1,1,1,4,1]]))\n",
    "src3 = Variable(torch.LongTensor([[1,2,3,4,5,6,5,4,3,2]]))\n",
    "src4 = Variable(torch.LongTensor([[1,2,3,2,1,2,3,2,1,2]]))\n",
    "src5 = Variable(torch.LongTensor([[1,10,1,10,1,10,1,10,1,10]]))\n",
    "src_mask = Variable(torch.ones(1, 1, 10) )\n",
    "print(transformer.greedy_decode(src1, src_mask, max_len=10, start_symbol=1))\n",
    "print(transformer.greedy_decode(src2, src_mask, max_len=10, start_symbol=1))\n",
    "print(transformer.greedy_decode(src3, src_mask, max_len=10, start_symbol=1))\n",
    "print(transformer.greedy_decode(src4, src_mask, max_len=10, start_symbol=1))\n",
    "print(transformer.greedy_decode(src5, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
