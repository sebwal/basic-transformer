{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "GLOBAL = {\n",
    "    'embedded_length': 512,\n",
    "    'continuity_length': 64,\n",
    "    'n_vocab': 29\n",
    "}\n",
    "\n",
    "# Transformer (Top level)\n",
    "TRANS_CONST = {\n",
    "    'n_attention_layers': 8,\n",
    "    'n_attention_heads': 8,    \n",
    "\n",
    "    'max_output_length': 6,\n",
    "    \n",
    "    'embedding_dic_size': GLOBAL['n_vocab'], \n",
    "    'embedded_vec_size': GLOBAL['embedded_length'],\n",
    "    \n",
    "    # 'pos_encoding_input': GLOBAL['embedded_length'],\n",
    "    # 'pos_encoding_output': GLOBAL['embedded_length'],\n",
    "    \n",
    "    'linear_input': GLOBAL['embedded_length'],\n",
    "    'linear_output': GLOBAL['n_vocab'] # output vocab size\n",
    "}\n",
    "\n",
    "# Encoder, EncoderLayer\n",
    "ENCODER_CONST = {\n",
    "    'norm1_size': GLOBAL['embedded_length'], # same as input matrix width\n",
    "    'norm2_size': GLOBAL['embedded_length'],\n",
    "\n",
    "    # maybe rename these two, it's just for knowing the input dim and the dim that the FF layer will work with\n",
    "    'ff1': GLOBAL['embedded_length'], \n",
    "    'ff2': GLOBAL['embedded_length'] * 4\n",
    "}\n",
    "\n",
    "# Decoder, DecoderLayer\n",
    "DECODER_CONST = {\n",
    "    'norm1_size': GLOBAL['embedded_length'], # same as input matrix width\n",
    "    'norm2_size': GLOBAL['embedded_length'],\n",
    "    'norm3_size': GLOBAL['embedded_length'],\n",
    "\n",
    "    'ff1': GLOBAL['embedded_length'],#TODO RENAME\n",
    "    'ff2': GLOBAL['embedded_length'] * 4#TODO RENAME\n",
    "}\n",
    "\n",
    "# MultiHeadAttention, SingleHeadAttention\n",
    "ATTENTION_CONST = {\n",
    "    'mh_concat_width': GLOBAL['continuity_length']*TRANS_CONST['n_attention_heads'], # single head attention width * number of heads\n",
    "    'mh_output_width': GLOBAL['embedded_length'], #TODO - I'm just guessing this. Didn't see in illustrated transformer. Since we have to use this for the add & norm layer though it has to be the same as the input width (I think)\n",
    "\n",
    "    # W_q weight matrix \n",
    "    'sh_linear1_input': GLOBAL['embedded_length'], # same as embedded length to end up with n_words x 64\n",
    "    'sh_linear1_output': GLOBAL['continuity_length'], # specified in the paper\n",
    "    # W_k weight matrix \n",
    "    'sh_linear2_input': GLOBAL['embedded_length'], # same as embedded length to end up with n_words x 64\n",
    "    'sh_linear2_output': GLOBAL['continuity_length'], # specified in the paper\n",
    "    # W_v weight matrix \n",
    "    'sh_linear3_input': GLOBAL['embedded_length'], # same as embedded length to end up with n_words x 64\n",
    "    'sh_linear3_output': GLOBAL['continuity_length'], # specified in the paper\n",
    "    \n",
    "    'sh_scale_factor': math.sqrt(GLOBAL['continuity_length']) # specified in the paper, square root of dimension of key vector/matrix (64)\n",
    "}\n",
    "\n",
    "# FeedForward\n",
    "FEEDFORWARD_CONST = {\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEEDFORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_model, dim_ff, dropout=FEEDFORWARD_CONST['dropout']):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim_model, dim_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_ff, dim_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.linear1(x)\n",
    "        x = nn.functional.relu(x) \n",
    "        x = self.dropout(x) \n",
    "        x = self.linear2(x) \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIHEADATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, masked=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.masked = masked\n",
    "        self.attentionHeads = nn.ModuleList([SingleHeadAttention(masked) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(ATTENTION_CONST['mh_concat_width'], ATTENTION_CONST['mh_output_width'])\n",
    "        self.lastHeadKV = None\n",
    "\n",
    "    def forward(self, inputs, encoderKV=None):\n",
    "        x = []\n",
    "        for head in self.attentionHeads:\n",
    "            sh_attention, k, v = head(inputs, encoderKV=encoderKV) \n",
    "            x.append(sh_attention)\n",
    "        self.lastHeadKV = {'K': k,'V': v}\n",
    "        x = torch.cat(x, 1) # concatinate all single head attention outputs\n",
    "        x = self.linear(x) # matmul with weight matrix (linear layer) to get 10x64 shape\n",
    "        return x\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, masked):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.masked = masked\n",
    "        self.linear1 = nn.Linear(ATTENTION_CONST['sh_linear1_input'], ATTENTION_CONST['sh_linear1_output'])\n",
    "        self.linear2 = nn.Linear(ATTENTION_CONST['sh_linear2_input'], ATTENTION_CONST['sh_linear2_output'])\n",
    "        self.linear3 = nn.Linear(ATTENTION_CONST['sh_linear3_input'], ATTENTION_CONST['sh_linear3_output'])\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([ATTENTION_CONST['sh_scale_factor']]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, encoderKV=None):        \n",
    "        q = self.linear1(inputs)\n",
    "        k = self.linear2(inputs) if encoderKV == None else encoderKV['K']\n",
    "        v = self.linear3(inputs) if encoderKV == None else encoderKV['V']\n",
    "        x = torch.matmul(q, k.permute(1, 0)) \n",
    "        x = x * self.scale\n",
    "        # if self.masked:\n",
    "        #     # TODO \"future positions\" have to be set to -inf. this is for the decoder to only allow self attention to consider earlier positions.\n",
    "        x = self.softmax(x) \n",
    "        x = torch.matmul(x, v)\n",
    "        return x if encoderKV != None else x, k, v\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers, n_attention_heads):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoderLayers = nn.ModuleList([DecoderLayer(n_attention_heads) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, inputs, encoderKV):\n",
    "        x = inputs\n",
    "        for layer in self.decoderLayers:\n",
    "            x = layer(x, encoderKV) \n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_attention_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mhattention_masked = MultiHeadAttention(n_attention_heads, masked=True)\n",
    "        self.mhattention = MultiHeadAttention(n_attention_heads)\n",
    "        self.feedforward = FeedForward(DECODER_CONST['ff1'], DECODER_CONST['ff2'])\n",
    "        self.norm1 = nn.LayerNorm(DECODER_CONST['norm1_size'])\n",
    "        self.norm2 = nn.LayerNorm(DECODER_CONST['norm2_size'])\n",
    "        self.norm3 = nn.LayerNorm(DECODER_CONST['norm3_size'])\n",
    "\n",
    "    def forward(self, inputs, encoderKV):\n",
    "        x = inputs\n",
    "        z = x\n",
    "        x = self.mhattention_masked(x) #TODO masking not implemented\n",
    "        x = z + x\n",
    "        x = self.norm1(x)\n",
    "        z = x\n",
    "        x = self.mhattention(x, encoderKV=encoderKV) \n",
    "        x = z + x\n",
    "        x = self.norm2(x)\n",
    "        z = x\n",
    "        x = self.feedforward(x)\n",
    "        x = z + x\n",
    "        x = self.norm3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layer, n_attention_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoderLayers = nn.ModuleList([EncoderLayer(n_attention_heads) for _ in range(n_layer)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        lastLayerKV = None\n",
    "        for layer in self.encoderLayers:\n",
    "            x = layer(x)\n",
    "            lastLayerKV = layer.lastLayerKV\n",
    "        return x, lastLayerKV\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_attention_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mhattention = MultiHeadAttention(n_attention_heads)\n",
    "        self.feedforward = FeedForward(ENCODER_CONST['ff1'], ENCODER_CONST['ff2'])\n",
    "        self.norm1 = nn.LayerNorm(ENCODER_CONST['norm1_size'])\n",
    "        self.norm2 = nn.LayerNorm(ENCODER_CONST['norm2_size'])\n",
    "        self.lastLayerKV = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs \n",
    "        z = x\n",
    "        x = self.mhattention(x)\n",
    "        self.lastLayerKV = self.mhattention.lastHeadKV\n",
    "        x = z + x\n",
    "        x = self.norm1(x)\n",
    "        z = x\n",
    "        x = self.feedforward(x)\n",
    "        x = z + x\n",
    "        x = self.norm2(x) \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_layers=TRANS_CONST['n_attention_layers'], n_attention_heads=TRANS_CONST['n_attention_heads']):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(n_layers, n_attention_heads)\n",
    "        self.decoder = Decoder(n_layers, n_attention_heads)\n",
    "        self.embedding = nn.Embedding(TRANS_CONST['embedding_dic_size'], TRANS_CONST['embedded_vec_size'])\n",
    "        # self.posEncoding = #TODO\n",
    "        self.linear = nn.Linear(TRANS_CONST['linear_input'], TRANS_CONST['linear_output'])\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def __call__(self, inputs=None):\n",
    "        if inputs != None: \n",
    "            raise NotImplementedError\n",
    "\n",
    "        import random\n",
    "        inputs = []\n",
    "        for _ in range(13): inputs.append(numpy.zeros(26)) # 26 is vocab size, should be constant; 13 is just a random amount of words in the sequence\n",
    "        inputs = torch.Tensor(inputs)\n",
    "        for i in inputs: i[random.randint(0, len(i) - 1)] = 1\n",
    "        return self.forward(inputs.long())\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # TODO FIRST PRIO\n",
    "            # get all inputs for embedding (real example for translation tasks etc, noise, decoder input) on the same format, which should be NxV\n",
    "        #### ENCODING ####\n",
    "        x = self.doEmbedding(inputs)\n",
    "        # x = self.posEncoding(x)\n",
    "        _, encoderKV = self.encoder(x) #TODO try running the encoder output trough 2 additional linear layers to make the KV matrices\n",
    "\n",
    "        #### DECODING ####\n",
    "        sos = numpy.zeros(GLOBAL['n_vocab'])\n",
    "        sos[0] = 1\n",
    "        x = torch.Tensor([sos])\n",
    "        ## Embedding\n",
    "        x = self.doEmbedding(x)\n",
    "        # x = self.posEncoding(x) #TODO\n",
    "        ## Decoding\n",
    "        while len(x) < TRANS_CONST['max_output_length']: #TODO add eos token\n",
    "            new_word = self.decoder(x, encoderKV)[0, :].unsqueeze(dim=0)\n",
    "            x = torch.cat([x, new_word], dim=0)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def doEmbedding(self, inputs):\n",
    "        inputs = inputs.nonzero()[:, 1] # this gets all indices of nonzero values from the inputs matrix\n",
    "        return self.embedding(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "#            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" % (i, loss / batch.ntokens.float(), tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    ret = total_loss / total_tokens.float()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 1: # Changed > 0 to > 1 because it throws errors otherwise            \n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seb/.local/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD9CAYAAABHnDf0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGmBJREFUeJzt3X2QXNV55/Hv0+8zPTNImpGQrHdsYSGwE2AKsL0kBOM1kF1Y1yaOVE7spBy03jVxdu3aLVzZwl62diuJU5s1FeKYxY5tKoHFjivWOrLlFMFlOzEY8RKQBIKxAGmQhN7fZjTT093P/nFvz/S0umdaUo9a9/bvU0x139uHnqd1pd+cOffcc83dERGReEm0uwAREWk9hbuISAwp3EVEYkjhLiISQwp3EZEYUriLiMTQrOFuZl81swNmtq3B62Zm95vZkJm9YGbXtL5MERE5G8303L8G3DrD67cBa8KvjcCXzr8sERE5H7OGu7v/CDgyQ5M7gW944ElgnpktaVWBIiJy9lox5r4U2FO1PRzuExGRNkm14D2szr66axqY2UaCoRvy+fy1a9eubcG3FxHpHM8888whd184W7tWhPswsLxqexmwt15Dd38QeBBgcHDQt27d2oJvLyLSOczsjWbatWJYZhPw0XDWzA3AcXff14L3FRGRczRrz93MHgFuAgbMbBj4HJAGcPe/ADYDtwNDwCjwO3NVrIiINGfWcHf3DbO87sAnW1aRiIicN12hKiISQwp3EZEYUriLiMSQwl1EJIYiF+5Pv36EP9myk2Kp3O5SREQuWpEL9+d3H+PPnhji9ESp3aWIiFy0IhfuuXRQ8tiEeu4iIo1EMNyTAIyp5y4i0pDCXUQkhiIX7l1huGvMXUSksciF+1TPXWPuIiKNRC7cuzJByeq5i4g0Frlwz6Y05i4iMpvIhXtXRuEuIjKb6IV75YRqQeEuItJI5MJdUyFFRGYXuXCfmgqp2TIiIo1ELtyzqcryA+q5i4g0ErlwTySMbCqhcBcRmUHkwh2CcXeFu4hIY5EM9650UhcxiYjMIJLhnksntPyAiMgMIhru6rmLiMwksuGuMXcRkcYiGe5dCncRkRlFM9wzGpYREZlJJMNdJ1RFRGYW0XBPauEwEZEZRDbcx4sKdxGRRiIZ7l3quYuIzCiS4Z5LJxgrlnH3dpciInJRimS4d6WTlMrOREnhLiJSTyTDffKGHRp3FxGpq6lwN7NbzWynmQ2Z2T11Xl9hZk+Y2XNm9oKZ3d76UqdMhrvG3UVE6po13M0sCTwA3AasAzaY2bqaZv8VeMzdrwbWA3/e6kKr5SbvxqRwFxGpp5me+3XAkLvvcvcC8ChwZ00bB/rC55cAe1tX4pm6Ju+jqguZRETqSTXRZimwp2p7GLi+ps3ngR+Y2e8BeeCWllTXQFcm+JmknruISH3N9Nytzr7aaSobgK+5+zLgduBhMzvjvc1so5ltNbOtBw8ePPtqQ7lUpeeucBcRqaeZcB8GlldtL+PMYZePA48BuPtPgRwwUPtG7v6guw+6++DChQvPrWIgl9GYu4jITJoJ96eBNWa22swyBCdMN9W02Q28H8DMriAI93Pvms+i0nMfV7iLiNQ1a7i7exG4G9gCvEQwK2a7md1nZneEzT4D3GVm/ww8Avy2z+Hlo13quYuIzKiZE6q4+2Zgc82+e6ue7wDe19rSGsulg59Jmi0jIlJfJK9QrUyF1OJhIiL1RTLctfyAiMjMIhnu2VQ4LKOeu4hIXZEMdzMjl07ohKqISAORDHcIxt11QlVEpL5Ih7t67iIi9UU23HPppJYfEBFpQOEuIhJDEQ73hMbcRUQaiGy4d2U05i4i0khkwz2X0rCMiEgj0Q139dxFRBqKbrinkrpCVUSkgciGe1cmwVhRJ1RFROqJbLjnUkmtCiki0kBkw70rk2SsWGIO7wkiIhJZkQ33XDqJO4xraEZE5AyRDneAcV3IJCJyhsiG++TdmDQdUkTkDJEN96n7qCrcRURqRTbc1XMXEWkssuE+eR9VhbuIyBkiH+7quYuInCnC4a4xdxGRRiIb7l2ZyrCMpkKKiNSKbLjnUuGwjJYgEBE5Q2TDfbLnXlS4i4jUimy4q+cuItJYdMM9E5SutWVERM4U2XDPJBMkTD13EZF6IhvuZkYurfuoiojUE9lwh2AJAl3EJCJypqbC3cxuNbOdZjZkZvc0aPNhM9thZtvN7K9bW2Z9Qc9dY+4iIrVSszUwsyTwAPABYBh42sw2ufuOqjZrgM8C73P3o2a2aK4KrpZLJzQsIyJSRzM99+uAIXff5e4F4FHgzpo2dwEPuPtRAHc/0Noy68tpWEZEpK5mwn0psKdqezjcV+1y4HIz+0cze9LMbm1VgTPp0glVEZG6Zh2WAazOvtq7UqeANcBNwDLgx2Z2lbsfm/ZGZhuBjQArVqw462Jr5dJJRgrF834fEZG4aabnPgwsr9peBuyt0+Y77j7h7q8BOwnCfhp3f9DdB919cOHCheda8ySdUBURqa+ZcH8aWGNmq80sA6wHNtW0+VvgVwDMbIBgmGZXKwutRydURUTqmzXc3b0I3A1sAV4CHnP37WZ2n5ndETbbAhw2sx3AE8B/dvfDc1V0hcbcRUTqa2bMHXffDGyu2Xdv1XMHPh1+XTBdGc2WERGpJ9JXqGr5ARGR+mIQ7mXK5drJOyIinS3i4a5lf0VE6ol0uHelK/dR1dCMiEi1SId7Lgx3nVQVEZku0uHeHd5HdVQ37BARmSbS4T6vOwPA0dFCmysREbm4RDrc+/NBuB8+pXAXEakW6XBfEIb7kRGFu4hItZiE+3ibKxERubhEOtxz6ST5TJLD6rmLiEwT6XAHWNCT0bCMiEiN6Id7PqtwFxGpEflw789nNFtGRKRG5MN9QT6jee4iIjUiH+79+QyHRwoES8qLiAjEINwX5DMUimVGtASBiMikWIQ7wBGNu4uITIp8uPf3hEsQ6EImEZFJkQ/3BfksoCUIRESqRT/cuys9d4W7iEhF9MO9R4uHiYjUiny45zNJMqmEwl1EpErkw93MdJWqiEiNyIc7BNMhteyviMiUGIW7eu4iIhWxCPfKEgQiIhKIRbhr2V8RkeliEe79PRlGCyXGJrS+jIgIxCTcK+vLaGhGRCQQq3DX4mEiIoFYhHt/XouHiYhUi0W4T/bcNSwjIgI0Ge5mdquZ7TSzITO7Z4Z2v2ZmbmaDrStxdv1aGVJEZJpZw93MksADwG3AOmCDma2r064X+BTwVKuLnE1fV4pUwhTuIiKhZnru1wFD7r7L3QvAo8Cdddr9d+CPgbEW1tcUM2O+rlIVEZnUTLgvBfZUbQ+H+yaZ2dXAcnf/bgtrOyu6SlVEZEoz4W519vnki2YJ4E+Bz8z6RmYbzWyrmW09ePBg81U2QevLiIhMaSbch4HlVdvLgL1V273AVcAPzex14AZgU72Tqu7+oLsPuvvgwoULz73qOhTuIiJTmgn3p4E1ZrbazDLAemBT5UV3P+7uA+6+yt1XAU8Cd7j71jmpuIFgTXfNcxcRgSbC3d2LwN3AFuAl4DF3325m95nZHXNdYLMW5LOcGCsyUSq3uxQRkbZLNdPI3TcDm2v23dug7U3nX9bZq9xL9ehIgUV9uXaUICJy0YjFFapQvQSBxt1FRGIT7gM9wVWqB09q3F1EJDbhvmJBNwBvHB5pcyUiIu0Xm3Bf1Jsll07w+uHRdpciItJ2sQn3RMJY1Z/n9UPquYuIxCbcAVb2d/O6hmVEROIV7qsG8uw5cppS2WdvLCISY7EK99X9eQqlMnuPnW53KSIibRWrcF/ZnwfQ0IyIdLxYhfvqgUq4a8aMiHS2WIX75HRIzZgRkQ4Xq3CvTIfUhUwi0uliFe4QTId8TT13EelwsQt3TYcUEYljuGs6pIhIPMMd4A3NmBGRDha/cB8IVod8TSdVRaSDxS7cL+3NkUsneEMnVUWkg8Uu3BMJY+WCvK5SFZGOFrtwh2BoRlepikgni2e49+fZfXhU0yFFpGPFM9wHgumQ+45rOqSIdKZYhvvK/mDGzOuHNDQjIp0pluH+jkU9ALy8/0SbKxERaY9Yhvui3hxL53Xx3O5j7S5FRKQtYhnuANesnM+zu4+2uwwRkbaIb7ivmMe+42M6qSoiHSnG4T4fgGff0NCMiHSe2Ib7FUv6yKYSGpoRkY4U23DPpBK8a+klCncR6UixDXcITqpuf/ME48VSu0sREbmg4h3uK+ZRKJXZvlfz3UWkszQV7mZ2q5ntNLMhM7unzuufNrMdZvaCmT1uZitbX+rZmzqpqqEZEekss4a7mSWBB4DbgHXABjNbV9PsOWDQ3d8NfAv441YXei4W9eliJhHpTM303K8Dhtx9l7sXgEeBO6sbuPsT7l5ZyOVJYFlryzx3uphJRDpRM+G+FNhTtT0c7mvk48D3zqeoVtLFTCLSiZoJd6uzr+5C6Wb2m8Ag8IUGr280s61mtvXgwYPNV3kedDGTiHSiZsJ9GFhetb0M2FvbyMxuAf4AuMPdx+u9kbs/6O6D7j64cOHCc6n3rF2xpI+ebIofv3phfpiIiFwMmgn3p4E1ZrbazDLAemBTdQMzuxr4MkGwH2h9mecuk0rwK2sX8YMdb+nOTCLSMWYNd3cvAncDW4CXgMfcfbuZ3Wdmd4TNvgD0AN80s+fNbFODt2uL265azJGRAj977Ui7SxERuSBSzTRy983A5pp991Y9v6XFdbXUTe9cSDaV4Pvb9vGet/e3uxwRkTkX6ytUK7ozKX758oVs2f4WZQ3NiEgH6IhwB7jtXYvZf2KM54c1a0ZE4q9jwv3mtZeSThpbtu1vdykiInOuY8L9kq407337AN/bth93Dc2ISLx1TLhDMGtm95FRduzTKpEiEm8dFe4fWHcpCYO/e2Ffu0sREZlTHRXu/T1Zbl67iEef3sPYhG7gISLx1VHhDvC7N17GkZEC3372zXaXIiIyZzou3K9fvYCrlvbx0E92ac67iMRWx4W7mXHXjZex6+AIT+y8qJbBERFpmY4Ld4Db37WEJZfkeOjHr7W7FBGROdGR4Z5OJvjt967ip7sOs+3N4+0uR0Sk5Toy3AHWX7eCfCbJn/9wqN2liIi0XMeG+yVdae76pcvY/OJ+/mnoULvLERFpqY4Nd4BP/PLbWbGgm3s3badQLLe7HBGRlunocM+lk3z+jnUMHTjFX/6jTq6KSHx0dLhDsFrkLVcs4ouPv8q+46fbXY6ISEt0fLgDfO5fX0mp7HzuO9u1YqSIxILCHVi+oJvP/MvL+cGOtzT3XURiQeEeuuvGy7jtqsX84fdf5p9+rtkzIhJtCveQmfGFX/8FVvV383t//Rx7j2n8XUSiS+FepSeb4su/Nch4scy/e/gZjp+eaHdJIiLnROFe4x2Lerh/wy/y8v4TfPQrTyngRSSSFO513Lz2Ur70kWt5ad9JfvOhpzg2Wmh3SSIiZ0Xh3sAt6y7ly791LTv3n2TD/3mK4aOj7S5JRCLK3RktFDl4cpw3Do9wfHTuRwSsXfO6BwcHfevWrW353mfjR68c5JN/9SyppHH/hqu5cc3CdpckInOsWCozMl7iVKHI6HiRU+NFRgul8LHIqfESo+NFRsbD54Uz24yMlxgJ940UilRH7f/40FV85PqV51SbmT3j7oOztlO4z+61QyN84uFneOXAST59y+X8+5veTiqpX3pELhblsjM6UQrDtsipseLk85EwjEfGq/aNB+F7anx6u9Fw33iTa02ZQT6TIp9Nks+k6A4f89kU3ZkkPdkU3ZXXsynymeDx6hXzWT2QP6fPqnBvsdFCkc9++0W+8/xerlrax//80Lt497J57S5LJLKCoYogTE+OTYXuyapgPlUVxqfGipwcn/5aJaRre8aNTAvjbIrebBDE+WwqDOIgkPM1zyuvTT7PBq91pZOY2dz/YU37DAr3lnN3Nr+4n8//v+0cPjXOR9+zik+9fw0L8pl2lyZywRRL5WmBfKoqeE+NFTk1PjFte6RQ1bb6sclATict7PUGwdqTCx+zQUj3ZNP0ZJP05KaCuNJ7nmozFdgXOoxbrdlwT12IYuLCzPjVdy/hxssH+ML3d/L1n77ON7fu4WPvXcVdN17GfIW8XMTKZedUIQziMIRPjE3fPjk2FcQnxyamBffJsO3pidKs38sMejKpycDtDQN5cV+O3qoQrn3ek02TzybpDR/z2RS5dPIC/OnEj3ru5+GVt05y/+Ov8ncv7qM7neTfXruMj1y/kncu7m13aRIzhWKZk2MTk8F7YmwqiE+OTVQFcBDYQRBPVAV18Dgbs+Bivt5sit5cerKX3JsLgzgztb83Vx3KlefBa93pJIlEtHvIFysNy1xAr7x1kr/44c/57ov7KBTLDK6cz4euWcoHr1zMQE+23eVJm41NlCZDuLpXfGKsyInT0/dVB/aJqufNnODLpRP0ZNP0VYI3l6I3m556ngte66kJ7r7c1HY+BsMWcadwb4OjIwX+5tlhHvnZbn5+cISEwXWrF/D+tZdy4+UDvPPSXv3DiRB3Z7xYnhbM1c9PVD1WhjZOjp8Z0M3c5SufSdKbS0/2kCth25dL0Te5P13Viw729VX1otOawdURWhruZnYr8EUgCTzk7n9Y83oW+AZwLXAY+A13f32m94xjuFe4OzvfOsnmF/fzvRf38eqBUwAM9GS5/rIFXL18HlevmM+Vb+vTeOIcKZc9nAI3PWjrjS9XQvpUnXCeKM3+7yOfSdLXlZ4WutVB3Vcd2tl03XBOaghDmtSycDezJPAK8AFgGHga2ODuO6ra/Afg3e7+CTNbD3zI3X9jpveNc7jX2nvsND8ZOsRPXj3EM28c5c1wxcmEwaqBPGsX97JmUS+rB/KsGsizckE387rTHdfLd3dOT1TmHpemzUmunpUxMj59Slz1TIzqGRyzSVTGl6f1iM8M6L4GoV35/xTMciG1crbMdcCQu+8K3/hR4E5gR1WbO4HPh8+/BfyZmZnrtkYAvG1eFx8eXM6HB5cDcODEGM/tOcb2vSfYuf8E2948wfe27Z82LawrnWTJJTmWzMsx0JNlYU+W/p4s87vTzOtOBz3FbDhOmk3SlU6SSyfn7FfzctkplMqMT5QZL5WCx2KZsYkS48USYxPB87GJMqcnSsFXocjpQpnRiSKnCyVGCyVOh1frjYZzk0cLja/ia6QyEyNfMy1ucV9ucqpcby4dnhSc2q4NcI0vS5w1E+5LgT1V28PA9Y3auHvRzI4D/YDuelHHor4cH7xyMR+8cvHkvrGJEsNHR3nt0Ci7j4yy79hp9h0fY9/x0zy3+xiHTo0zWph9CloqYWRSCTKpBOlkgnTCSCSMVMJImEHwH2ZG2R0cyu6UHUplp+xOsewUS2WKJWeiXGai5JTK5/5zOpNK0JVOks8k6cokJ6/YW9yXo6veVXzZFD3hlX6V+cn5qmlzmokhMrtmwr3ev6Laf+nNtMHMNgIbAVasWNHEt+4cuXSSdyzq5R2LGk+jHC0UOX56gmOjwdfkFX3jRcYngl7x6YkShWKZiVKZQhjQpXIQ2GX34KA4OI6ZYUDCjGTCMIOkGalkglTCSCWNTDL8IZFMTP7QyKQS5FIJsukkmWSCXDpBLvzNoStd+S0iQVcmeK6lGkQuvGbCfRhYXrW9DNjboM2wmaWAS4AjtW/k7g8CD0Iw5n4uBXey7kzQw11ySVe7SxGRi1wzXaqngTVmttrMMsB6YFNNm03Ax8Lnvwb8g8bbRUTaZ9aeeziGfjewhWAq5FfdfbuZ3QdsdfdNwFeAh81siKDHvn4uixYRkZk1tbaMu28GNtfsu7fq+Rjw660tTUREzpXOdImIxJDCXUQkhhTuIiIxpHAXEYkhhbuISAy1bclfMzsIvHEW/8sAnbmcgT535+nUz67P3ZyV7r5wtkZtC/ezZWZbm1kJLW70uTtPp352fe7W0rCMiEgMKdxFRGIoSuH+YLsLaBN97s7TqZ9dn7uFIjPmLiIizYtSz11ERJoUiXA3s1vNbKeZDZnZPe2uZ66Y2XIze8LMXjKz7Wb2++H+BWb292b2avg4v921zgUzS5rZc2b23XB7tZk9FX7u/xsuOR0rZjbPzL5lZi+Hx/09nXC8zew/hX/Ht5nZI2aWi+PxNrOvmtkBM9tWta/u8bXA/WHOvWBm15zP977owz28QfcDwG3AOmCDma1rb1Vzpgh8xt2vAG4APhl+1nuAx919DfB4uB1Hvw+8VLX9R8Cfhp/7KPDxtlQ1t74IfN/d1wK/QPD5Y328zWwp8Clg0N2vIlhKfD3xPN5fA26t2dfo+N4GrAm/NgJfOp9vfNGHO1U36Hb3AlC5QXfsuPs+d382fH6S4B/6UoLP+/Ww2deBf9OeCueOmS0DfhV4KNw24GaCG65DDD+3mfUBv0RwPwTcveDux+iA402w3HhXeOe2bmAfMTze7v4jzrwrXaPjeyfwDQ88CcwzsyXn+r2jEO71btC9tE21XDBmtgq4GngKuNTd90HwAwBY1L7K5sz/Bv4LUA63+4Fj7l4Mt+N43C8DDgJ/GQ5HPWRmeWJ+vN39TeBPgN0EoX4ceIb4H++KRse3pVkXhXBv6ubbcWJmPcDfAP/R3U+0u565Zmb/Cjjg7s9U767TNG7HPQVcA3zJ3a8GRojZEEw94RjzncBq4G1AnmBIolbcjvdsWvp3Pgrh3swNumPDzNIEwf5X7v7tcPdblV/PwscD7apvjrwPuMPMXicYdruZoCc/L/y1HeJ53IeBYXd/Ktz+FkHYx/143wK85u4H3X0C+DbwXuJ/vCsaHd+WZl0Uwr2ZG3THQjjO/BXgJXf/X1UvVd+A/GPAdy50bXPJ3T/r7svcfRXB8f0Hd/8I8ATBDdchnp97P7DHzN4Z7no/sIOYH2+C4ZgbzKw7/Dtf+dyxPt5VGh3fTcBHw1kzNwDHK8M358TdL/ov4HbgFeDnwB+0u545/Jz/guDXsBeA58Ov2wnGnx8HXg0fF7S71jn8M7gJ+G74/DLgZ8AQ8E0g2+765uDz/iKwNTzmfwvM74TjDfw34GVgG/AwkI3j8QYeITivMEHQM/94o+NLMCzzQJhzLxLMJjrn760rVEVEYigKwzIiInKWFO4iIjGkcBcRiSGFu4hIDCncRURiSOEuIhJDCncRkRhSuIuIxND/B40THZ/9zeSbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "crit = LabelSmoothing(5, 0, 0.1)\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
    "                                 ])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict.log()),\n",
    "                 Variable(torch.LongTensor([1]))).data[0]\n",
    "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10))).long()\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        norm = norm.float() # because it throws errors if it's not casted to float\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        ret = loss.data[0] * norm\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a4e39323c406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     run_epoch(data_gen(V, 30, 20), model, \n\u001b[0;32m---> 13\u001b[0;31m               SimpleLossCompute(model, criterion, model_opt))\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ad02d1851985>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         out = model.forward(batch.src, batch.trg, \n\u001b[0;32m----> 9\u001b[0;31m                             batch.src_mask, batch.trg_mask)\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train the simple copy task.\n",
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = Transformer() #make_model(V, V, N=2)\n",
    "model_opt = NoamOpt(512, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    run_epoch(data_gen(V, 30, 20), model, \n",
    "              SimpleLossCompute(model, criterion, model_opt))\n",
    "    model.eval()\n",
    "\n",
    "    print('epoch ' + str(epoch + 1))\n",
    "    print(run_epoch(data_gen(V, 30, 5), model, \n",
    "                    SimpleLossCompute(model.generator, criterion, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "model.eval()\n",
    "src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
    "#src = Variable(torch.LongTensor([[1,1, 1, 3, 3, 1, 1, 1, 4, 1]]) )\n",
    "src_mask = Variable(torch.ones(1, 1, 10) )\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1611, grad_fn=<BinaryCrossEntropyBackward>) tensor([[0.0227, 0.0354, 0.0307, 0.0195, 0.0447, 0.0190, 0.0168, 0.0422, 0.0368,\n",
      "         0.0412, 0.0472, 0.0276, 0.0494, 0.0172, 0.0656, 0.0245, 0.0471, 0.0612,\n",
      "         0.1000, 0.0223, 0.0625, 0.0138, 0.0224, 0.0245, 0.0512, 0.0087, 0.0151,\n",
      "         0.0116, 0.0191],\n",
      "        [0.0273, 0.0292, 0.0275, 0.0187, 0.0638, 0.0339, 0.0151, 0.0375, 0.0474,\n",
      "         0.0425, 0.0292, 0.0266, 0.0286, 0.0315, 0.0756, 0.0134, 0.0254, 0.0936,\n",
      "         0.0401, 0.0312, 0.0424, 0.0110, 0.0720, 0.0271, 0.0306, 0.0132, 0.0266,\n",
      "         0.0211, 0.0179],\n",
      "        [0.0167, 0.0302, 0.0193, 0.0229, 0.0456, 0.0393, 0.0211, 0.0585, 0.0644,\n",
      "         0.0513, 0.0276, 0.0237, 0.0396, 0.0219, 0.0500, 0.0136, 0.0350, 0.0972,\n",
      "         0.0370, 0.0168, 0.0456, 0.0125, 0.0625, 0.0275, 0.0385, 0.0093, 0.0351,\n",
      "         0.0210, 0.0162],\n",
      "        [0.0190, 0.0365, 0.0231, 0.0178, 0.0376, 0.0247, 0.0168, 0.0360, 0.0493,\n",
      "         0.0501, 0.0249, 0.0288, 0.0526, 0.0381, 0.0522, 0.0166, 0.0416, 0.0990,\n",
      "         0.0347, 0.0276, 0.0609, 0.0116, 0.0362, 0.0221, 0.0446, 0.0101, 0.0549,\n",
      "         0.0197, 0.0131],\n",
      "        [0.0201, 0.0334, 0.0230, 0.0300, 0.0278, 0.0645, 0.0270, 0.0401, 0.0383,\n",
      "         0.0365, 0.0520, 0.0190, 0.0254, 0.0507, 0.0390, 0.0156, 0.0262, 0.0814,\n",
      "         0.0367, 0.0397, 0.0470, 0.0074, 0.0523, 0.0306, 0.0360, 0.0247, 0.0341,\n",
      "         0.0201, 0.0216],\n",
      "        [0.0206, 0.0321, 0.0187, 0.0123, 0.0470, 0.0416, 0.0260, 0.0435, 0.0436,\n",
      "         0.0320, 0.0245, 0.0321, 0.0328, 0.0411, 0.0465, 0.0144, 0.0278, 0.1156,\n",
      "         0.0386, 0.0249, 0.0518, 0.0080, 0.0589, 0.0211, 0.0380, 0.0151, 0.0524,\n",
      "         0.0251, 0.0136]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.1117, grad_fn=<BinaryCrossEntropyBackward>) tensor([[0.0650, 0.0550, 0.0506, 0.0316, 0.0701, 0.0308, 0.0164, 0.0386, 0.0328,\n",
      "         0.0338, 0.0383, 0.0258, 0.0423, 0.0154, 0.0531, 0.0193, 0.0402, 0.0525,\n",
      "         0.0705, 0.0180, 0.0543, 0.0127, 0.0203, 0.0232, 0.0377, 0.0085, 0.0141,\n",
      "         0.0113, 0.0176],\n",
      "        [0.0372, 0.1072, 0.1110, 0.1204, 0.0795, 0.1480, 0.0191, 0.0338, 0.0187,\n",
      "         0.0127, 0.0132, 0.0207, 0.0207, 0.0191, 0.0253, 0.0071, 0.0292, 0.0212,\n",
      "         0.0115, 0.0083, 0.0260, 0.0179, 0.0136, 0.0163, 0.0062, 0.0138, 0.0099,\n",
      "         0.0191, 0.0133],\n",
      "        [0.0381, 0.1085, 0.1121, 0.1174, 0.0825, 0.1412, 0.0199, 0.0333, 0.0191,\n",
      "         0.0119, 0.0137, 0.0224, 0.0209, 0.0185, 0.0260, 0.0070, 0.0282, 0.0221,\n",
      "         0.0136, 0.0084, 0.0223, 0.0186, 0.0131, 0.0167, 0.0064, 0.0135, 0.0108,\n",
      "         0.0192, 0.0146],\n",
      "        [0.0386, 0.1074, 0.1095, 0.1085, 0.0840, 0.1461, 0.0188, 0.0334, 0.0195,\n",
      "         0.0126, 0.0136, 0.0217, 0.0226, 0.0192, 0.0270, 0.0074, 0.0323, 0.0210,\n",
      "         0.0125, 0.0079, 0.0252, 0.0183, 0.0140, 0.0166, 0.0065, 0.0128, 0.0106,\n",
      "         0.0175, 0.0147],\n",
      "        [0.0382, 0.1081, 0.1119, 0.1142, 0.0806, 0.1458, 0.0188, 0.0336, 0.0192,\n",
      "         0.0123, 0.0143, 0.0225, 0.0223, 0.0182, 0.0261, 0.0075, 0.0277, 0.0223,\n",
      "         0.0119, 0.0080, 0.0247, 0.0196, 0.0135, 0.0167, 0.0058, 0.0137, 0.0103,\n",
      "         0.0181, 0.0141],\n",
      "        [0.0379, 0.1065, 0.1173, 0.1127, 0.0799, 0.1403, 0.0202, 0.0355, 0.0191,\n",
      "         0.0121, 0.0138, 0.0212, 0.0238, 0.0187, 0.0266, 0.0070, 0.0288, 0.0233,\n",
      "         0.0133, 0.0080, 0.0246, 0.0171, 0.0140, 0.0166, 0.0059, 0.0140, 0.0101,\n",
      "         0.0173, 0.0142]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "transformer = Transformer()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0002)\n",
    "loss = torch.nn.BCELoss()\n",
    "\n",
    "real_sample = torch.Tensor([\n",
    "        [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0]\n",
    "])\n",
    "real_sample = torch.Tensor([\n",
    "        [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "        [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0, 0,0],\n",
    "])\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    sample = transformer()        \n",
    "    # target = torch.ones(sample.shape[0], sample.shape[1])\n",
    "    error = loss(sample, real_sample)\n",
    "    if _ % 10 == 0: print(error, sample)\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    if(error > 8): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "tensor([0, 5, 5, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(real_sample, dim=1))\n",
    "print(torch.argmax(sample, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
